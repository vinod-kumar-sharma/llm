{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d44d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20481\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "317aebf1-58cc-4673-98a1-343a0f1b0258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication', '.', '\"', 'Of', 'course', 'it', \"'\", 's', 'going', 'to', 'send', 'the', 'value', 'of', 'my', 'picture', \"'\", 'way', 'up', ';', 'but', 'I', 'don', \"'\", 't', 'think', 'of', 'that', ',', 'Mr', '.', 'Rickham', '--', 'the', 'loss', 'to', 'Arrt', 'is', 'all', 'I', 'think', 'of', '.', '\"', 'The', 'word', ',', 'on', 'Mrs', '.', 'Thwing', \"'\", 's', 'lips', ',', 'multiplied', 'its', '_', 'rs', '_', 'as', 'though', 'they', 'were', 'reflected', 'in', 'an', 'endless', 'vista', 'of', 'mirrors', '.', 'And', 'it', 'was', 'not', 'only', 'the', 'Mrs', '.', 'Thwings', 'who', 'mourned', '.', 'Had', 'not', 'the', 'exquisite', 'Hermia', 'Croft', ',', 'at', 'the', 'last', 'Grafton', 'Gallery', 'show', ',', 'stopped', 'me', 'before', 'Gisburn', \"'\", 's', '\"', 'Moon-dancers', '\"', 'to', 'say', ',', 'with', 'tears', 'in', 'her', 'eyes', ':', '\"', 'We', 'shall', 'not', 'look', 'upon', 'its', 'like', 'again', '\"', '?', 'Well', '!', '--', 'even', 'through', 'the', 'prism', 'of', 'Hermia', \"'\", 's', 'tears', 'I', 'felt', 'able', 'to', 'face', 'the', 'fact', 'with', 'equanimity', '.', 'Poor', 'Jack', 'Gisburn', '!', 'The', 'women', 'had', 'made', 'him', '--', 'it', 'was', 'fitting', 'that', 'they', 'should', 'mourn', 'him', '.', 'Among', 'his', 'own', 'sex', 'fewer', 'regrets', 'were', 'heard', ',', 'and', 'in', 'his', 'own', 'trade', 'hardly', 'a', 'murmur', '.', 'Professional', 'jealousy', '?', 'Perhaps', '.', 'If', 'it', 'were', ',', 'the', 'honour', 'of', 'the', 'craft', 'was', 'vindicated', 'by', 'little', 'Claude', 'Nutley', ',', 'who', ',', 'in', 'all', 'good', 'faith', ',', 'brought', 'out', 'in', 'the', 'Burlington', 'a', 'very', 'handsome', '\"', 'obituary', '\"', 'on', 'Jack', '--', 'one', 'of', 'those', 'showy', 'articles', 'stocked', 'with', 'random', 'technicalities', 'that', 'I', 'have', 'heard', '(', 'I', 'won', \"'\", 't', 'say', 'by', 'whom', ')', 'compared', 'to', 'Gisburn', \"'\", 's', 'painting', '.', 'And', 'so', '--', 'his', 'resolve', 'being', 'apparently', 'irrevocable', '--', 'the', 'discussion', 'gradually', 'died', 'out', ',', 'and', ',', 'as', 'Mrs', '.', 'Thwing', 'had', 'predicted', ',', 'the', 'price', 'of', '\"', 'Gisburns', '\"', 'went', 'up', '.', 'It', 'was', 'not', 'till', 'three', 'years', 'later', 'that', ',', 'in', 'the', 'course', 'of', 'a', 'few', 'weeks', \"'\", 'idling', 'on', 'the', 'Riviera', ',', 'it', 'suddenly', 'occurred', 'to', 'me', 'to', 'wonder', 'why', 'Gisburn', 'had', 'given', 'up', 'his', 'painting', '.', 'On', 'reflection', ',', 'it', 'really', 'was', 'a', 'tempting', 'problem', '.', 'To', 'accuse', 'his', 'wife', 'would', 'have', 'been', 'too', 'easy', '--', 'his', 'fair', 'sitters', 'had', 'been', 'denied', 'the', 'solace', 'of', 'saying', 'that', 'Mrs', '.', 'Gisburn', 'had', '\"', 'dragged', 'him', 'down', '.', '\"', 'For', 'Mrs', '.', 'Gisburn', '--', 'as', 'such', '--', 'had', 'not', 'existed', 'till', 'nearly', 'a', 'year', 'after', 'Jack', \"'\", 's', 'resolve', 'had', 'been', 'taken', '.', 'It', 'might', 'be', 'that', 'he', 'had', 'married', 'her', '--', 'since', 'he', 'liked', 'his', 'ease', '--', 'because', 'he', 'didn', \"'\", 't', 'want', 'to', 'go', 'on', 'painting', ';', 'but', 'it', 'would', 'have', 'been', 'hard', 'to', 'prove', 'that', 'he', 'had', 'given', 'up', 'his', 'painting', 'because', 'he', 'had', 'married', 'her', '.', 'Of', 'course', ',', 'if', 'she', 'had', 'not', 'dragged', 'him', 'down', ',', 'she', 'had', 'equally', ',', 'as', 'Miss', 'Croft', 'contended', ',', 'failed', 'to', '\"', 'lift', 'him', 'up', '\"', '--', 'she', 'had', 'not', 'led', 'him', 'back', 'to', 'the', 'easel', '.', 'To', 'put', 'the', 'brush', 'into', 'his', 'hand', 'again', '--', 'what', 'a', 'vocation', 'for', 'a', 'wife', '!', 'But', 'Mrs', '.', 'Gisburn', 'appeared', 'to', 'have', 'disdained', 'it', '--', 'and', 'I', 'felt', 'it', 'might', 'be', 'interesting', 'to', 'find', 'out', 'why', '.', 'The', 'desultory', 'life', 'of', 'the', 'Riviera', 'lends', 'itself', 'to', 'such', 'purely', 'academic', 'speculations', ';', 'and', 'having', ',', 'on', 'my', 'way', 'to', 'Monte', 'Carlo', ',', 'caught', 'a', 'glimpse', 'of', 'Jack', \"'\", 's', 'balustraded', 'terraces', 'between', 'the', 'pines', ',', 'I', 'had', 'myself', 'borne', 'thither', 'the', 'next', 'day', '.', 'I', 'found', 'the', 'couple', 'at', 'tea', 'beneath', 'their', 'palm-trees', ';', 'and', 'Mrs', '.', 'Gisburn', \"'\", 's', 'welcome', 'was', 'so', 'genial', 'that', ',', 'in', 'the', 'ensuing', 'weeks', ',', 'I', 'claimed', 'it', 'frequently', '.', 'It', 'was', 'not', 'that', 'my', 'hostess', 'was', '\"', 'interesting', '\"', ':', 'on', 'that', 'point', 'I', 'could', 'have', 'given', 'Miss', 'Croft', 'the', 'fullest', 'reassurance', '.', 'It', 'was', 'just', 'because', 'she', 'was', '_', 'not', '_', 'interesting', '--', 'if', 'I', 'may', 'be', 'pardoned', 'the', 'bull', '--', 'that', 'I', 'found', 'her', 'so', '.', 'For', 'Jack', ',', 'all', 'his', 'life', ',', 'had', 'been', 'surrounded', 'by', 'interesting', 'women', ':', 'they', 'had', 'fostered', 'his', 'art', ',', 'it', 'had', 'been', 'reared', 'in', 'the', 'hot-house', 'of', 'their', 'adulation', '.', 'And', 'it', 'was', 'therefore', 'instructive', 'to', 'note', 'what', 'effect', 'the', '\"', 'deadening', 'atmosphere', 'of', 'mediocrity', '\"', '(', 'I', 'quote', 'Miss', 'Croft', ')', 'was', 'having', 'on', 'him', '.', 'I', 'have', 'mentioned', 'that', 'Mrs', '.', 'Gisburn', 'was', 'rich', ';', 'and', 'it', 'was', 'immediately', 'perceptible', 'that', 'her', 'husband', 'was', 'extracting', 'from', 'this', 'circumstance', 'a', 'delicate', 'but', 'substantial', 'satisfaction', '.', 'It', 'is', ',', 'as', 'a', 'rule', ',', 'the', 'people', 'who', 'scorn', 'money', 'who', 'get', 'most', 'out', 'of', 'it', ';', 'and', 'Jack', \"'\", 's', 'elegant', 'disdain', 'of', 'his', 'wife', \"'\", 's', 'big', 'balance', 'enabled', 'him', ',', 'with', 'an', 'appearance', 'of', 'perfect', 'good-breeding', ',', 'to', 'transmute', 'it', 'into', 'objects', 'of', 'art', 'and', 'luxury', '.', 'To', 'the', 'latter', ',', 'I', 'must', 'add', ',', 'he', 'remained', 'relatively', 'indifferent', ';', 'but', 'he', 'was', 'buying', 'Renaissance', 'bronzes', 'and', 'eighteenth-century', 'pictures', 'with', 'a', 'discrimination', 'that', 'bespoke', 'the', 'amplest', 'resources', '.', '\"', 'Money', \"'\", 's', 'only', 'excuse', 'is', 'to', 'put', 'beauty', 'into', 'circulation', ',', '\"', 'was', 'one', 'of', 'the', 'axioms', 'he', 'laid', 'down', 'across', 'the', 'Sevres', 'and', 'silver', 'of', 'an', 'exquisitely', 'appointed', 'luncheon-table', ',', 'when', ',', 'on', 'a', 'later', 'day', ',', 'I', 'had', 'again', 'run', 'over', 'from', 'Monte', 'Carlo', ';', 'and', 'Mrs', '.', 'Gisburn', ',', 'beaming', 'on', 'him', ',', 'added', 'for', 'my', 'enlightenment', ':', '\"', 'Jack', 'is', 'so', 'morbidly', 'sensitive', 'to', 'every', 'form', 'of', 'beauty', '.', '\"', 'Poor', 'Jack', '!', 'It', 'had', 'always', 'been', 'his', 'fate', 'to', 'have', 'women', 'say', 'such', 'things', 'of', 'him', ':', 'the', 'fact', 'should', 'be', 'set', 'down', 'in', 'extenuation', '.', 'What', 'struck', 'me', 'now', 'was', 'that', ',', 'for', 'the', 'first', 'time', ',', 'he', 'resented', 'the', 'tone', '.', 'I', 'had', 'seen', 'him', ',', 'so', 'often', ',', 'basking', 'under', 'similar', 'tributes', '--', 'was', 'it', 'the', 'conjugal', 'note', 'that', 'robbed', 'them', 'of', 'their', 'savour', '?', 'No', '--', 'for', ',', 'oddly', 'enough', ',', 'it', 'became', 'apparent', 'that', 'he', 'was', 'fond', 'of', 'Mrs', '.', 'Gisburn', '--', 'fond', 'enough', 'not', 'to', 'see', 'her', 'absurdity', '.', 'It', 'was', 'his', 'own', 'absurdity', 'he', 'seemed', 'to', 'be', 'wincing', 'under', '--', 'his', 'own', 'attitude', 'as', 'an', 'object', 'for', 'garlands', 'and', 'incense', '.', '\"', 'My', 'dear', ',', 'since', 'I', \"'\", 've', 'chucked', 'painting', 'people', 'don', \"'\", 't', 'say', 'that', 'stuff', 'about', 'me', '--', 'they', 'say', 'it', 'about', 'Victor', 'Grindle', ',', '\"', 'was', 'his', 'only', 'protest', ',', 'as', 'he', 'rose', 'from', 'the', 'table', 'and', 'strolled', 'out', 'onto', 'the', 'sunlit', 'terrace', '.', 'I', 'glanced', 'after', 'him', ',', 'struck', 'by', 'his', 'last', 'word', '.', 'Victor', 'Grindle', 'was', ',', 'in', 'fact', ',', 'becoming', 'the', 'man', 'of', 'the', 'moment', '--', 'as', 'Jack', 'himself', ',', 'one', 'might', 'put', 'it', ',', 'had', 'been', 'the', 'man', 'of', 'the', 'hour', '.', 'The', 'younger', 'artist', 'was', 'said', 'to', 'have', 'formed', 'himself', 'at', 'my', 'friend', \"'\", 's', 'feet', ',', 'and', 'I', 'wondered', 'if', 'a', 'tinge', 'of', 'jealousy', 'underlay', 'the', 'latter', \"'\", 's', 'mysterious', 'abdication', '.', 'But', 'no', '--', 'for', 'it', 'was', 'not', 'till', 'after', 'that', 'event', 'that', 'the', '_', 'rose', 'Dubarry', '_', 'drawing-rooms', 'had', 'begun', 'to', 'display', 'their', '\"', 'Grindles', '.', '\"', 'I', 'turned', 'to', 'Mrs', '.', 'Gisburn', ',', 'who', 'had', 'lingered', 'to', 'give', 'a', 'lump', 'of', 'sugar', 'to', 'her', 'spaniel', 'in', 'the', 'dining-room', '.', '\"', 'Why', '_', 'has', '_', 'he', 'chucked', 'painting', '?', '\"', 'I', 'asked', 'abruptly', '.', 'She', 'raised', 'her', 'eyebrows', 'with', 'a', 'hint', 'of', 'good-humoured', 'surprise', '.', '\"', 'Oh', ',', 'he', 'doesn', \"'\", 't', '_', 'have', '_', 'to', 'now', ',', 'you', 'know', ';', 'and', 'I', 'want', 'him', 'to', 'enjoy', 'himself', ',', '\"', 'she', 'said', 'quite', 'simply', '.', 'I', 'looked', 'about', 'the', 'spacious', 'white-panelled', 'room', ',', 'with', 'its', '_', 'famille-verte', '_', 'vases', 'repeating', 'the', 'tones', 'of', 'the', 'pale', 'damask', 'curtains', ',', 'and', 'its', 'eighteenth-century', 'pastels', 'in', 'delicate', 'faded', 'frames', '.', '\"', 'Has', 'he', 'chucked', 'his', 'pictures', 'too', '?', 'I', 'haven', \"'\", 't', 'seen', 'a', 'single', 'one', 'in', 'the', 'house', '.', '\"', 'A', 'slight', 'shade', 'of', 'constraint', 'crossed', 'Mrs', '.', 'Gisburn', \"'\", 's', 'open', 'countenance', '.', '\"', 'It', \"'\", 's', 'his', 'ridiculous', 'modesty', ',', 'you', 'know', '.', 'He', 'says', 'they', \"'\", 're', 'not', 'fit', 'to', 'have', 'about', ';', 'he', \"'\", 's', 'sent', 'them', 'all', 'away', 'except', 'one', '--', 'my', 'portrait', '--', 'and', 'that', 'I', 'have', 'to', 'keep', 'upstairs', '.', '\"', 'His', 'ridiculous', 'modesty', '--', 'Jack', \"'\", 's', 'modesty', 'about', 'his', 'pictures', '?', 'My', 'curiosity', 'was', 'growing', 'like', 'the', 'bean-stalk', '.', 'I', 'said', 'persuasively', 'to', 'my', 'hostess', ':', '\"', 'I', 'must', 'really', 'see', 'your', 'portrait', ',', 'you', 'know', '.', '\"', 'She', 'glanced', 'out', 'almost', 'timorously', 'at', 'the', 'terrace', 'where', 'her', 'husband', ',', 'lounging', 'in', 'a', 'hooded', 'chair', ',', 'had', 'lit', 'a', 'cigar', 'and', 'drawn', 'the', 'Russian', 'deerhound', \"'\", 's', 'head', 'between', 'his', 'knees', '.', '\"', 'Well', ',', 'come', 'while', 'he', \"'\", 's', 'not', 'looking', ',', '\"', 'she', 'said', ',', 'with', 'a', 'laugh', 'that', 'tried', 'to', 'hide', 'her', 'nervousness', ';', 'and', 'I', 'followed', 'her', 'between', 'the', 'marble', 'Emperors', 'of', 'the', 'hall', ',', 'and', 'up', 'the', 'wide', 'stairs', 'with', 'terra-cotta', 'nymphs', 'poised', 'among', 'flowers', 'at', 'each', 'landing', '.', 'In', 'the', 'dimmest', 'corner', 'of', 'her', 'boudoir', ',', 'amid', 'a', 'profusion', 'of', 'delicate', 'and', 'distinguished', 'objects', ',', 'hung', 'one', 'of', 'the', 'familiar', 'oval', 'canvases', ',', 'in', 'the', 'inevitable', 'garlanded', 'frame', '.', 'The', 'mere', 'outline', 'of', 'the', 'frame', 'called', 'up', 'all', 'Gisburn', \"'\", 's', 'past', '!', 'Mrs', '.', 'Gisburn', 'drew', 'back', 'the', 'window-curtains', ',', 'moved', 'aside', 'a', '_', 'jardiniere', '_', 'full', 'of', 'pink', 'azaleas', ',', 'pushed', 'an', 'arm-chair', 'away', ',', 'and', 'said', ':', '\"', 'If', 'you', 'stand', 'here', 'you', 'can', 'just', 'manage', 'to', 'see', 'it', '.', 'I', 'had', 'it', 'over', 'the', 'mantel-piece', ',', 'but', 'he', 'wouldn', \"'\", 't', 'let', 'it', 'stay', '.', '\"', 'Yes', '--', 'I', 'could', 'just', 'manage', 'to', 'see', 'it', '--', 'the', 'first', 'portrait', 'of', 'Jack', \"'\", 's', 'I', 'had', 'ever', 'had', 'to', 'strain', 'my', 'eyes', 'over', '!', 'Usually', 'they', 'had', 'the', 'place', 'of', 'honour', '--', 'say', 'the', 'central', 'panel', 'in', 'a', 'pale', 'yellow', 'or', '_', 'rose', 'Dubarry', '_', 'drawing-room', ',', 'or', 'a', 'monumental', 'easel', 'placed', 'so', 'that', 'it', 'took', 'the', 'light', 'through', 'curtains', 'of', 'old', 'Venetian', 'point', '.', 'The', 'more', 'modest', 'place', 'became', 'the', 'picture', 'better', ';', 'yet', ',', 'as', 'my', 'eyes', 'grew', 'accustomed', 'to', 'the', 'half-light', ',', 'all', 'the', 'characteristic', 'qualities', 'came', 'out', '--', 'all', 'the', 'hesitations', 'disguised', 'as', 'audacities', ',', 'the', 'tricks', 'of', 'prestidigitation', 'by', 'which', ',', 'with', 'such', 'consummate', 'skill', ',', 'he', 'managed', 'to', 'divert', 'attention', 'from', 'the', 'real', 'business', 'of', 'the', 'picture', 'to', 'some', 'pretty', 'irrelevance', 'of', 'detail', '.', 'Mrs', '.', 'Gisburn', ',', 'presenting', 'a', 'neutral', 'surface', 'to', 'work', 'on', '--', 'forming', ',', 'as', 'it', 'were', ',', 'so', 'inevitably', 'the', 'background', 'of', 'her', 'own', 'picture', '--', 'had', 'lent', 'herself', 'in', 'an', 'unusual', 'degree', 'to', 'the', 'display', 'of', 'this', 'false', 'virtuosity', '.', 'The', 'picture', 'was', 'one', 'of', 'Jack', \"'\", 's', '\"', 'strongest', ',', '\"', 'as', 'his', 'admirers', 'would', 'have', 'put', 'it', '--', 'it', 'represented', ',', 'on', 'his', 'part', ',', 'a', 'swelling', 'of', 'muscles', ',', 'a', 'congesting', 'of', 'veins', ',', 'a', 'balancing', ',', 'straddling', 'and', 'straining', ',', 'that', 'reminded', 'one', 'of', 'the', 'circus-clown', \"'\", 's', 'ironic', 'efforts', 'to', 'lift', 'a', 'feather', '.', 'It', 'met', ',', 'in', 'short', ',', 'at', 'every', 'point', 'the', 'demand', 'of', 'lovely', 'woman', 'to', 'be', 'painted', '\"', 'strongly', '\"', 'because', 'she', 'was', 'tired', 'of', 'being', 'painted', '\"', 'sweetly', '\"', '--', 'and', 'yet', 'not', 'to', 'lose', 'an', 'atom', 'of', 'the', 'sweetness', '.', '\"', 'It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.', '\"', 'The', 'last', 'but', 'one', ',', '\"', 'she', 'corrected', 'herself', '--', '\"', 'but', 'the', 'other', 'doesn', \"'\", 't', 'count', ',', 'because', 'he', 'destroyed', 'it', '.', '\"', '\"', 'Destroyed', 'it', '?', '\"', 'I', 'was', 'about', 'to', 'follow', 'up', 'this', 'clue', 'when', 'I', 'heard', 'a', 'footstep', 'and', 'saw', 'Jack', 'himself', 'on', 'the', 'threshold', '.', 'As', 'he', 'stood', 'there', ',', 'his', 'hands', 'in', 'the', 'pockets', 'of', 'his', 'velveteen', 'coat', ',', 'the', 'thin', 'brown', 'waves', 'of', 'hair', 'pushed', 'back', 'from', 'his', 'white', 'forehead', ',', 'his', 'lean', 'sunburnt', 'cheeks', 'furrowed', 'by', 'a', 'smile', 'that', 'lifted', 'the', 'tips', 'of', 'a', 'self-confident', 'moustache', ',', 'I', 'felt', 'to', 'what', 'a', 'degree', 'he', 'had', 'the', 'same', 'quality', 'as', 'his', 'pictures', '--', 'the', 'quality', 'of', 'looking', 'cleverer', 'than', 'he', 'was', '.', 'His', 'wife', 'glanced', 'at', 'him', 'deprecatingly', ',', 'but', 'his', 'eyes', 'travelled', 'past', 'her', 'to', 'the', 'portrait', '.', '\"', 'Mr', '.', 'Rickham', 'wanted', 'to', 'see', 'it', ',', '\"', 'she', 'began', ',', 'as', 'if', 'excusing', 'herself', '.', 'He', 'shrugged', 'his', 'shoulders', ',', 'still', 'smiling', '.', '\"', 'Oh', ',', 'Rickham', 'found', 'me', 'out', 'long', 'ago', ',', '\"', 'he', 'said', 'lightly', ';', 'then', ',', 'passing', 'his', 'arm', 'through', 'mine', ':', '\"', 'Come', 'and', 'see', 'the', 'rest', 'of', 'the', 'house', '.', '\"', 'He', 'showed', 'it', 'to', 'me', 'with', 'a', 'kind', 'of', 'naive', 'suburban', 'pride', ':', 'the', 'bath-rooms', ',', 'the', 'speaking-tubes', ',', 'the', 'dress-closets', ',', 'the', 'trouser-presses', '--', 'all', 'the', 'complex', 'simplifications', 'of', 'the', 'millionaire', \"'\", 's', 'domestic', 'economy', '.', 'And', 'whenever', 'my', 'wonder', 'paid', 'the', 'expected', 'tribute', 'he', 'said', ',', 'throwing', 'out', 'his', 'chest', 'a', 'little', ':', '\"', 'Yes', ',', 'I', 'really', 'don', \"'\", 't', 'see', 'how', 'people', 'manage', 'to', 'live', 'without', 'that', '.', '\"', 'Well', '--', 'it', 'was', 'just', 'the', 'end', 'one', 'might', 'have', 'foreseen', 'for', 'him', '.', 'Only', 'he', 'was', ',', 'through', 'it', 'all', 'and', 'in', 'spite', 'of', 'it', 'all', '--', 'as', 'he', 'had', 'been', 'through', ',', 'and', 'in', 'spite', 'of', ',', 'his', 'pictures', '--', 'so', 'handsome', ',', 'so', 'charming', ',', 'so', 'disarming', ',', 'that', 'one', 'longed', 'to', 'cry', 'out', ':', '\"', 'Be', 'dissatisfied', 'with', 'your', 'leisure', '!', '\"', 'as', 'once', 'one', 'had', 'longed', 'to', 'say', ':', '\"', 'Be', 'dissatisfied', 'with', 'your', 'work', '!', '\"', 'But', ',', 'with', 'the', 'cry', 'on', 'my', 'lips', ',', 'my', 'diagnosis', 'suffered', 'an', 'unexpected', 'check', '.', '\"', 'This', 'is', 'my', 'own', 'lair', ',', '\"', 'he', 'said', ',', 'leading', 'me', 'into', 'a', 'dark', 'plain', 'room', 'at', 'the', 'end', 'of', 'the', 'florid', 'vista', '.', 'It', 'was', 'square', 'and', 'brown', 'and', 'leathery', ':', 'no', '\"', 'effects', '\"', ';', 'no', 'bric-a-brac', ',', 'none', 'of', 'the', 'air', 'of', 'posing', 'for', 'reproduction', 'in', 'a', 'picture', 'weekly', '--', 'above', 'all', ',', 'no', 'least', 'sign', 'of', 'ever', 'having', 'been', 'used', 'as', 'a', 'studio', '.', 'The', 'fact', 'brought', 'home', 'to', 'me', 'the', 'absolute', 'finality', 'of', 'Jack', \"'\", 's', 'break', 'with', 'his', 'old', 'life', '.', '\"', 'Don', \"'\", 't', 'you', 'ever', 'dabble', 'with', 'paint', 'any', 'more', '?', '\"', 'I', 'asked', ',', 'still', 'looking', 'about', 'for', 'a', 'trace', 'of', 'such', 'activity', '.', '\"', 'Never', ',', '\"', 'he', 'said', 'briefly', '.', '\"', 'Or', 'water-colour', '--', 'or', 'etching', '?', '\"', 'His', 'confident', 'eyes', 'grew', 'dim', ',', 'and', 'his', 'cheeks', 'paled', 'a', 'little', 'under', 'their', 'handsome', 'sunburn', '.', '\"', 'Never', 'think', 'of', 'it', ',', 'my', 'dear', 'fellow', '--', 'any', 'more', 'than', 'if', 'I', \"'\", 'd', 'never', 'touched', 'a', 'brush', '.', '\"', 'And', 'his', 'tone', 'told', 'me', 'in', 'a', 'flash', 'that', 'he', 'never', 'thought', 'of', 'anything', 'else', '.', 'I', 'moved', 'away', ',', 'instinctively', 'embarrassed', 'by', 'my', 'unexpected', 'discovery', ';', 'and', 'as', 'I', 'turned', ',', 'my', 'eye', 'fell', 'on', 'a', 'small', 'picture', 'above', 'the', 'mantel-piece', '--', 'the', 'only', 'object', 'breaking', 'the', 'plain', 'oak', 'panelling', 'of', 'the', 'room', '.', '\"', 'Oh', ',', 'by', 'Jove', '!', '\"', 'I', 'said', '.', 'It', 'was', 'a', 'sketch', 'of', 'a', 'donkey', '--', 'an', 'old', 'tired', 'donkey', ',', 'standing', 'in', 'the', 'rain', 'under', 'a', 'wall', '.', '\"', 'By', 'Jove', '--', 'a', 'Stroud', '!', '\"', 'I', 'cried', '.', 'He', 'was', 'silent', ';', 'but', 'I', 'felt', 'him', 'close', 'behind', 'me', ',', 'breathing', 'a', 'little', 'quickly', '.', '\"', 'What', 'a', 'wonder', '!', 'Made', 'with', 'a', 'dozen', 'lines', '--', 'but', 'on', 'everlasting', 'foundations', '.', 'You', 'lucky', 'chap', ',', 'where', 'did', 'you', 'get', 'it', '?', '\"', 'He', 'answered', 'slowly', ':', '\"', 'Mrs', '.', 'Stroud', 'gave', 'it', 'to', 'me', '.', '\"', '\"', 'Ah', '--', 'I', 'didn', \"'\", 't', 'know', 'you', 'even', 'knew', 'the', 'Strouds', '.', 'He', 'was', 'such', 'an', 'inflexible', 'hermit', '.', '\"', '\"', 'I', 'didn', \"'\", 't', '--', 'till', 'after', '.', '.', '.', '.', 'She', 'sent', 'for', 'me', 'to', 'paint', 'him', 'when', 'he', 'was', 'dead', '.', '\"', '\"', 'When', 'he', 'was', 'dead', '?', 'You', '?', '\"', 'I', 'must', 'have', 'let', 'a', 'little', 'too', 'much', 'amazement', 'escape', 'through', 'my', 'surprise', ',', 'for', 'he', 'answered', 'with', 'a', 'deprecating', 'laugh', ':', '\"', 'Yes', '--', 'she', \"'\", 's', 'an', 'awful', 'simpleton', ',', 'you', 'know', ',', 'Mrs', '.', 'Stroud', '.', 'Her', 'only', 'idea', 'was', 'to', 'have', 'him', 'done', 'by', 'a', 'fashionable', 'painter', '--', 'ah', ',', 'poor', 'Stroud', '!', 'She', 'thought', 'it', 'the', 'surest', 'way', 'of', 'proclaiming', 'his', 'greatness', '--', 'of', 'forcing', 'it', 'on', 'a', 'purblind', 'public', '.', 'And', 'at', 'the', 'moment', 'I', 'was', '_', 'the', '_', 'fashionable', 'painter', '.', '\"', '\"', 'Ah', ',', 'poor', 'Stroud', '--', 'as', 'you', 'say', '.', 'Was', '_', 'that', '_', 'his', 'history', '?', '\"', '\"', 'That', 'was', 'his', 'history', '.', 'She', 'believed', 'in', 'him', ',', 'gloried', 'in', 'him', '--', 'or', 'thought', 'she', 'did', '.', 'But', 'she', 'couldn', \"'\", 't', 'bear', 'not', 'to', 'have', 'all', 'the', 'drawing-rooms', 'with', 'her', '.', 'She', 'couldn', \"'\", 't', 'bear', 'the', 'fact', 'that', ',', 'on', 'varnishing', 'days', ',', 'one', 'could', 'always', 'get', 'near', 'enough', 'to', 'see', 'his', 'pictures', '.', 'Poor', 'woman', '!', 'She', \"'\", 's', 'just', 'a', 'fragment', 'groping', 'for', 'other', 'fragments', '.', 'Stroud', 'is', 'the', 'only', 'whole', 'I', 'ever', 'knew', '.', '\"', '\"', 'You', 'ever', 'knew', '?', 'But', 'you', 'just', 'said', '--', '\"', 'Gisburn', 'had', 'a', 'curious', 'smile', 'in', 'his', 'eyes', '.', '\"', 'Oh', ',', 'I', 'knew', 'him', ',', 'and', 'he', 'knew', 'me', '--', 'only', 'it', 'happened', 'after', 'he', 'was', 'dead', '.', '\"', 'I', 'dropped', 'my', 'voice', 'instinctively', '.', '\"', 'When', 'she', 'sent', 'for', 'you', '?', '\"', '\"', 'Yes', '--', 'quite', 'insensible', 'to', 'the', 'irony', '.', 'She', 'wanted', 'him', 'vindicated', '--', 'and', 'by', 'me', '!', '\"', 'He', 'laughed', 'again', ',', 'and', 'threw', 'back', 'his', 'head', 'to', 'look', 'up', 'at', 'the', 'sketch', 'of', 'the', 'donkey', '.', '\"', 'There', 'were', 'days', 'when', 'I', 'couldn', \"'\", 't', 'look', 'at', 'that', 'thing', '--', 'couldn', \"'\", 't', 'face', 'it', '.', 'But', 'I', 'forced', 'myself', 'to', 'put', 'it', 'here', ';', 'and', 'now', 'it', \"'\", 's', 'cured', 'me', '--', 'cured', 'me', '.', 'That', \"'\", 's', 'the', 'reason', 'why', 'I', 'don', \"'\", 't', 'dabble', 'any', 'more', ',', 'my', 'dear', 'Rickham', ';', 'or', 'rather', 'Stroud', 'himself', 'is', 'the', 'reason', '.', '\"', 'For', 'the', 'first', 'time', 'my', 'idle', 'curiosity', 'about', 'my', 'companion', 'turned', 'into', 'a', 'serious', 'desire', 'to', 'understand', 'him', 'better', '.', '\"', 'I', 'wish', 'you', \"'\", 'd', 'tell', 'me', 'how', 'it', 'happened', ',', '\"', 'I', 'said', '.', 'He', 'stood', 'looking', 'up', 'at', 'the', 'sketch', ',', 'and', 'twirling', 'between', 'his', 'fingers', 'a', 'cigarette', 'he', 'had', 'forgotten', 'to', 'light', '.', 'Suddenly', 'he', 'turned', 'toward', 'me', '.', '\"', 'I', \"'\", 'd', 'rather', 'like', 'to', 'tell', 'you', '--', 'because', 'I', \"'\", 've', 'always', 'suspected', 'you', 'of', 'loathing', 'my', 'work', '.', '\"', 'I', 'made', 'a', 'deprecating', 'gesture', ',', 'which', 'he', 'negatived', 'with', 'a', 'good-humoured', 'shrug', '.', '\"', 'Oh', ',', 'I', 'didn', \"'\", 't', 'care', 'a', 'straw', 'when', 'I', 'believed', 'in', 'myself', '--', 'and', 'now', 'it', \"'\", 's', 'an', 'added', 'tie', 'between', 'us', '!', '\"', 'He', 'laughed', 'slightly', ',', 'without', 'bitterness', ',', 'and', 'pushed', 'one', 'of', 'the', 'deep', 'arm-chairs', 'forward', '.', '\"', 'There', ':', 'make', 'yourself', 'comfortable', '--', 'and', 'here', 'are', 'the', 'cigars', 'you', 'like', '.', '\"', 'He', 'placed', 'them', 'at', 'my', 'elbow', 'and', 'continued', 'to', 'wander', 'up', 'and', 'down', 'the', 'room', ',', 'stopping', 'now', 'and', 'then', 'beneath', 'the', 'picture', '.', '\"', 'How', 'it', 'happened', '?', 'I', 'can', 'tell', 'you', 'in', 'five', 'minutes', '--', 'and', 'it', 'didn', \"'\", 't', 'take', 'much', 'longer', 'to', 'happen', '.', '.', '.', '.', 'I', 'can', 'remember', 'now', 'how', 'surprised', 'and', 'pleased', 'I', 'was', 'when', 'I', 'got', 'Mrs', '.', 'Stroud', \"'\", 's', 'note', '.', 'Of', 'course', ',', 'deep', 'down', ',', 'I', 'had', 'always', '_', 'felt', '_', 'there', 'was', 'no', 'one', 'like', 'him', '--', 'only', 'I', 'had', 'gone', 'with', 'the', 'stream', ',', 'echoed', 'the', 'usual', 'platitudes', 'about', 'him', ',', 'till', 'I', 'half', 'got', 'to', 'think', 'he', 'was', 'a', 'failure', ',', 'one', 'of', 'the', 'kind', 'that', 'are', 'left', 'behind', '.', 'By', 'Jove', ',', 'and', 'he', '_', 'was', '_', 'left', 'behind', '--', 'because', 'he', 'had', 'come', 'to', 'stay', '!', 'The', 'rest', 'of', 'us', 'had', 'to', 'let', 'ourselves', 'be', 'swept', 'along', 'or', 'go', 'under', ',', 'but', 'he', 'was', 'high', 'above', 'the', 'current', '--', 'on', 'everlasting', 'foundations', ',', 'as', 'you', 'say', '.', '\"', 'Well', ',', 'I', 'went', 'off', 'to', 'the', 'house', 'in', 'my', 'most', 'egregious', 'mood', '--', 'rather', 'moved', ',', 'Lord', 'forgive', 'me', ',', 'at', 'the', 'pathos', 'of', 'poor', 'Stroud', \"'\", 's', 'career', 'of', 'failure', 'being', 'crowned', 'by', 'the', 'glory', 'of', 'my', 'painting', 'him', '!', 'Of', 'course', 'I', 'meant', 'to', 'do', 'the', 'picture', 'for', 'nothing', '--', 'I', 'told', 'Mrs', '.', 'Stroud', 'so', 'when', 'she', 'began', 'to', 'stammer', 'something', 'about', 'her', 'poverty', '.', 'I', 'remember', 'getting', 'off', 'a', 'prodigious', 'phrase', 'about', 'the', 'honour', 'being', '_', 'mine', '_', '--', 'oh', ',', 'I', 'was', 'princely', ',', 'my', 'dear', 'Rickham', '!', 'I', 'was', 'posing', 'to', 'myself', 'like', 'one', 'of', 'my', 'own', 'sitters', '.', '\"', 'Then', 'I', 'was', 'taken', 'up', 'and', 'left', 'alone', 'with', 'him', '.', 'I', 'had', 'sent', 'all', 'my', 'traps', 'in', 'advance', ',', 'and', 'I', 'had', 'only', 'to', 'set', 'up', 'the', 'easel', 'and', 'get', 'to', 'work', '.', 'He', 'had', 'been', 'dead', 'only', 'twenty-four', 'hours', ',', 'and', 'he', 'died', 'suddenly', ',', 'of', 'heart', 'disease', ',', 'so', 'that', 'there', 'had', 'been', 'no', 'preliminary', 'work', 'of', 'destruction', '--', 'his', 'face', 'was', 'clear', 'and', 'untouched', '.', 'I', 'had', 'met', 'him', 'once', 'or', 'twice', ',', 'years', 'before', ',', 'and', 'thought', 'him', 'insignificant', 'and', 'dingy', '.', 'Now', 'I', 'saw', 'that', 'he', 'was', 'superb', '.', '\"', 'I', 'was', 'glad', 'at', 'first', ',', 'with', 'a', 'merely', 'aesthetic', 'satisfaction', ':', 'glad', 'to', 'have', 'my', 'hand', 'on', 'such', 'a', \"'\", 'subject', '.', \"'\", 'Then', 'his', 'strange', 'life-likeness', 'began', 'to', 'affect', 'me', 'queerly', '--', 'as', 'I', 'blocked', 'the', 'head', 'in', 'I', 'felt', 'as', 'if', 'he', 'were', 'watching', 'me', 'do', 'it', '.', 'The', 'sensation', 'was', 'followed', 'by', 'the', 'thought', ':', 'if', 'he', '_', 'were', '_', 'watching', 'me', ',', 'what', 'would', 'he', 'say', 'to', 'my', 'way', 'of', 'working', '?', 'My', 'strokes', 'began', 'to', 'go', 'a', 'little', 'wild', '--', 'I', 'felt', 'nervous', 'and', 'uncertain', '.', '\"', 'Once', ',', 'when', 'I', 'looked', 'up', ',', 'I', 'seemed', 'to', 'see', 'a', 'smile', 'behind', 'his', 'close', 'grayish', 'beard', '--', 'as', 'if', 'he', 'had', 'the', 'secret', ',', 'and', 'were', 'amusing', 'himself', 'by', 'holding', 'it', 'back', 'from', 'me', '.', 'That', 'exasperated', 'me', 'still', 'more', '.', 'The', 'secret', '?', 'Why', ',', 'I', 'had', 'a', 'secret', 'worth', 'twenty', 'of', 'his', '!', 'I', 'dashed', 'at', 'the', 'canvas', 'furiously', ',', 'and', 'tried', 'some', 'of', 'my', 'bravura', 'tricks', '.', 'But', 'they', 'failed', 'me', ',', 'they', 'crumbled', '.', 'I', 'saw', 'that', 'he', 'wasn', \"'\", 't', 'watching', 'the', 'showy', 'bits', '--', 'I', 'couldn', \"'\", 't', 'distract', 'his', 'attention', ';', 'he', 'just', 'kept', 'his', 'eyes', 'on', 'the', 'hard', 'passages', 'between', '.', 'Those', 'were', 'the', 'ones', 'I', 'had', 'always', 'shirked', ',', 'or', 'covered', 'up', 'with', 'some', 'lying', 'paint', '.', 'And', 'how', 'he', 'saw', 'through', 'my', 'lies', '!', '\"', 'I', 'looked', 'up', 'again', ',', 'and', 'caught', 'sight', 'of', 'that', 'sketch', 'of', 'the', 'donkey', 'hanging', 'on', 'the', 'wall', 'near', 'his', 'bed', '.', 'His', 'wife', 'told', 'me', 'afterward', 'it', 'was', 'the', 'last', 'thing', 'he', 'had', 'done', '--', 'just', 'a', 'note', 'taken', 'with', 'a', 'shaking', 'hand', ',', 'when', 'he', 'was', 'down', 'in', 'Devonshire', 'recovering', 'from', 'a', 'previous', 'heart', 'attack', '.', 'Just', 'a', 'note', '!', 'But', 'it', 'tells', 'his', 'whole', 'history', '.', 'There', 'are', 'years', 'of', 'patient', 'scornful', 'persistence', 'in', 'every', 'line', '.', 'A', 'man', 'who', 'had', 'swum', 'with', 'the', 'current', 'could', 'never', 'have', 'learned', 'that', 'mighty', 'up-stream', 'stroke', '.', '.', '.', '.', '\"', 'I', 'turned', 'back', 'to', 'my', 'work', ',', 'and', 'went', 'on', 'groping', 'and', 'muddling', ';', 'then', 'I', 'looked', 'at', 'the', 'donkey', 'again', '.', 'I', 'saw', 'that', ',', 'when', 'Stroud', 'laid', 'in', 'the', 'first', 'stroke', ',', 'he', 'knew', 'just', 'what', 'the', 'end', 'would', 'be', '.', 'He', 'had', 'possessed', 'his', 'subject', ',', 'absorbed', 'it', ',', 'recreated', 'it', '.', 'When', 'had', 'I', 'done', 'that', 'with', 'any', 'of', 'my', 'things', '?', 'They', 'hadn', \"'\", 't', 'been', 'born', 'of', 'me', '--', 'I', 'had', 'just', 'adopted', 'them', '.', '.', '.', '.', '\"', 'Hang', 'it', ',', 'Rickham', ',', 'with', 'that', 'face', 'watching', 'me', 'I', 'couldn', \"'\", 't', 'do', 'another', 'stroke', '.', 'The', 'plain', 'truth', 'was', ',', 'I', 'didn', \"'\", 't', 'know', 'where', 'to', 'put', 'it', '--', '_', 'I', 'had', 'never', 'known', '_', '.', 'Only', ',', 'with', 'my', 'sitters', 'and', 'my', 'public', ',', 'a', 'showy', 'splash', 'of', 'colour', 'covered', 'up', 'the', 'fact', '--', 'I', 'just', 'threw', 'paint', 'into', 'their', 'faces', '.', '.', '.', '.', 'Well', ',', 'paint', 'was', 'the', 'one', 'medium', 'those', 'dead', 'eyes', 'could', 'see', 'through', '--', 'see', 'straight', 'to', 'the', 'tottering', 'foundations', 'underneath', '.', 'Don', \"'\", 't', 'you', 'know', 'how', ',', 'in', 'talking', 'a', 'foreign', 'language', ',', 'even', 'fluently', ',', 'one', 'says', 'half', 'the', 'time', 'not', 'what', 'one', 'wants', 'to', 'but', 'what', 'one', 'can', '?', 'Well', '--', 'that', 'was', 'the', 'way', 'I', 'painted', ';', 'and', 'as', 'he', 'lay', 'there', 'and', 'watched', 'me', ',', 'the', 'thing', 'they', 'called', 'my', \"'\", 'technique', \"'\", 'collapsed', 'like', 'a', 'house', 'of', 'cards', '.', 'He', 'didn', \"'\", 't', 'sneer', ',', 'you', 'understand', ',', 'poor', 'Stroud', '--', 'he', 'just', 'lay', 'there', 'quietly', 'watching', ',', 'and', 'on', 'his', 'lips', ',', 'through', 'the', 'gray', 'beard', ',', 'I', 'seemed', 'to', 'hear', 'the', 'question', ':', \"'\", 'Are', 'you', 'sure', 'you', 'know', 'where', 'you', \"'\", 're', 'coming', 'out', '?', \"'\", '\"', 'If', 'I', 'could', 'have', 'painted', 'that', 'face', ',', 'with', 'that', 'question', 'on', 'it', ',', 'I', 'should', 'have', 'done', 'a', 'great', 'thing', '.', 'The', 'next', 'greatest', 'thing', 'was', 'to', 'see', 'that', 'I', 'couldn', \"'\", 't', '--', 'and', 'that', 'grace', 'was', 'given', 'me', '.', 'But', ',', 'oh', ',', 'at', 'that', 'minute', ',', 'Rickham', ',', 'was', 'there', 'anything', 'on', 'earth', 'I', 'wouldn', \"'\", 't', 'have', 'given', 'to', 'have', 'Stroud', 'alive', 'before', 'me', ',', 'and', 'to', 'hear', 'him', 'say', ':', \"'\", 'It', \"'\", 's', 'not', 'too', 'late', '--', 'I', \"'\", 'll', 'show', 'you', 'how', \"'\", '?', '\"', 'It', '_', 'was', '_', 'too', 'late', '--', 'it', 'would', 'have', 'been', ',', 'even', 'if', 'he', \"'\", 'd', 'been', 'alive', '.', 'I', 'packed', 'up', 'my', 'traps', ',', 'and', 'went', 'down', 'and', 'told', 'Mrs', '.', 'Stroud', '.', 'Of', 'course', 'I', 'didn', \"'\", 't', 'tell', 'her', '_', 'that', '_', '--', 'it', 'would', 'have', 'been', 'Greek', 'to', 'her', '.', 'I', 'simply', 'said', 'I', 'couldn', \"'\", 't', 'paint', 'him', ',', 'that', 'I', 'was', 'too', 'moved', '.', 'She', 'rather', 'liked', 'the', 'idea', '--', 'she', \"'\", 's', 'so', 'romantic', '!', 'It', 'was', 'that', 'that', 'made', 'her', 'give', 'me', 'the', 'donkey', '.', 'But', 'she', 'was', 'terribly', 'upset', 'at', 'not', 'getting', 'the', 'portrait', '--', 'she', 'did', 'so', 'want', 'him', \"'\", 'done', \"'\", 'by', 'some', 'one', 'showy', '!', 'At', 'first', 'I', 'was', 'afraid', 'she', 'wouldn', \"'\", 't', 'let', 'me', 'off', '--', 'and', 'at', 'my', 'wits', \"'\", 'end', 'I', 'suggested', 'Grindle', '.', 'Yes', ',', 'it', 'was', 'I', 'who', 'started', 'Grindle', ':', 'I', 'told', 'Mrs', '.', 'Stroud', 'he', 'was', 'the', \"'\", 'coming', \"'\", 'man', ',', 'and', 'she', 'told', 'somebody', 'else', ',', 'and', 'so', 'it', 'got', 'to', 'be', 'true', '.', '.', '.', '.', 'And', 'he', 'painted', 'Stroud', 'without', 'wincing', ';', 'and', 'she', 'hung', 'the', 'picture', 'among', 'her', 'husband', \"'\", 's', 'things', '.', '.', '.', '.', '\"', 'He', 'flung', 'himself', 'down', 'in', 'the', 'arm-chair', 'near', 'mine', ',', 'laid', 'back', 'his', 'head', ',', 'and', 'clasping', 'his', 'arms', 'beneath', 'it', ',', 'looked', 'up', 'at', 'the', 'picture', 'above', 'the', 'chimney-piece', '.', '\"', 'I', 'like', 'to', 'fancy', 'that', 'Stroud', 'himself', 'would', 'have', 'given', 'it', 'to', 'me', ',', 'if', 'he', \"'\", 'd', 'been', 'able', 'to', 'say', 'what', 'he', 'thought', 'that', 'day', '.', '\"', 'And', ',', 'in', 'answer', 'to', 'a', 'question', 'I', 'put', 'half-mechanically', '--', '\"', 'Begin', 'again', '?', '\"', 'he', 'flashed', 'out', '.', '\"', 'When', 'the', 'one', 'thing', 'that', 'brings', 'me', 'anywhere', 'near', 'him', 'is', 'that', 'I', 'knew', 'enough', 'to', 'leave', 'off', '?', '\"', 'He', 'stood', 'up', 'and', 'laid', 'his', 'hand', 'on', 'my', 'shoulder', 'with', 'a', 'laugh', '.', '\"', 'Only', 'the', 'irony', 'of', 'it', 'is', 'that', 'I', '_', 'am', '_', 'still', 'painting', '--', 'since', 'Grindle', \"'\", 's', 'doing', 'it', 'for', 'me', '!', 'The', 'Strouds', 'stand', 'alone', ',', 'and', 'happen', 'once', '--', 'but', 'there', \"'\", 's', 'no', 'exterminating', 'our', 'kind', 'of', 'art', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = [t for t in re.split(r'([,.:;?_!\"()\\']|--|\\s+)', raw_text) if t and not t.isspace()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14f81f92-2ede-4345-9658-8790f20d6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(tokens))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1c12915-0f8d-4e40-8ad8-64be44a9513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "print(len(all_words))\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49075ca6-4597-44f5-b037-d7f11b22038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129, '<|endoftext|>': 1130, '<|unk|>': 1131}\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n",
      "('How', 52)\n",
      "('I', 53)\n",
      "('If', 54)\n",
      "('In', 55)\n",
      "('It', 56)\n",
      "('Jack', 57)\n",
      "('Jove', 58)\n",
      "('Just', 59)\n",
      "('Lord', 60)\n",
      "('Made', 61)\n",
      "('Miss', 62)\n",
      "('Money', 63)\n",
      "('Monte', 64)\n",
      "('Moon-dancers', 65)\n",
      "('Mr', 66)\n",
      "('Mrs', 67)\n",
      "('My', 68)\n",
      "('Never', 69)\n",
      "('No', 70)\n",
      "('Now', 71)\n",
      "('Nutley', 72)\n",
      "('Of', 73)\n",
      "('Oh', 74)\n",
      "('On', 75)\n",
      "('Once', 76)\n",
      "('Only', 77)\n",
      "('Or', 78)\n",
      "('Perhaps', 79)\n",
      "('Poor', 80)\n",
      "('Professional', 81)\n",
      "('Renaissance', 82)\n",
      "('Rickham', 83)\n",
      "('Riviera', 84)\n",
      "('Rome', 85)\n",
      "('Russian', 86)\n",
      "('Sevres', 87)\n",
      "('She', 88)\n",
      "('Stroud', 89)\n",
      "('Strouds', 90)\n",
      "('Suddenly', 91)\n",
      "('That', 92)\n",
      "('The', 93)\n",
      "('Then', 94)\n",
      "('There', 95)\n",
      "('They', 96)\n",
      "('This', 97)\n",
      "('Those', 98)\n",
      "('Though', 99)\n",
      "('Thwing', 100)\n",
      "('Thwings', 101)\n",
      "('To', 102)\n",
      "('Usually', 103)\n",
      "('Venetian', 104)\n",
      "('Victor', 105)\n",
      "('Was', 106)\n",
      "('We', 107)\n",
      "('Well', 108)\n",
      "('What', 109)\n",
      "('When', 110)\n",
      "('Why', 111)\n",
      "('Yes', 112)\n",
      "('You', 113)\n",
      "('_', 114)\n",
      "('a', 115)\n",
      "('abdication', 116)\n",
      "('able', 117)\n",
      "('about', 118)\n",
      "('above', 119)\n",
      "('abruptly', 120)\n",
      "('absolute', 121)\n",
      "('absorbed', 122)\n",
      "('absurdity', 123)\n",
      "('academic', 124)\n",
      "('accuse', 125)\n",
      "('accustomed', 126)\n",
      "('across', 127)\n",
      "('activity', 128)\n",
      "('add', 129)\n",
      "('added', 130)\n",
      "('admirers', 131)\n",
      "('adopted', 132)\n",
      "('adulation', 133)\n",
      "('advance', 134)\n",
      "('aesthetic', 135)\n",
      "('affect', 136)\n",
      "('afraid', 137)\n",
      "('after', 138)\n",
      "('afterward', 139)\n",
      "('again', 140)\n",
      "('ago', 141)\n",
      "('ah', 142)\n",
      "('air', 143)\n",
      "('alive', 144)\n",
      "('all', 145)\n",
      "('almost', 146)\n",
      "('alone', 147)\n",
      "('along', 148)\n",
      "('always', 149)\n",
      "('am', 150)\n",
      "('amazement', 151)\n",
      "('amid', 152)\n",
      "('among', 153)\n",
      "('amplest', 154)\n",
      "('amusing', 155)\n",
      "('an', 156)\n",
      "('and', 157)\n",
      "('another', 158)\n",
      "('answer', 159)\n",
      "('answered', 160)\n",
      "('any', 161)\n",
      "('anything', 162)\n",
      "('anywhere', 163)\n",
      "('apparent', 164)\n",
      "('apparently', 165)\n",
      "('appearance', 166)\n",
      "('appeared', 167)\n",
      "('appointed', 168)\n",
      "('are', 169)\n",
      "('arm', 170)\n",
      "('arm-chair', 171)\n",
      "('arm-chairs', 172)\n",
      "('arms', 173)\n",
      "('art', 174)\n",
      "('articles', 175)\n",
      "('artist', 176)\n",
      "('as', 177)\n",
      "('aside', 178)\n",
      "('asked', 179)\n",
      "('at', 180)\n",
      "('atmosphere', 181)\n",
      "('atom', 182)\n",
      "('attack', 183)\n",
      "('attention', 184)\n",
      "('attitude', 185)\n",
      "('audacities', 186)\n",
      "('away', 187)\n",
      "('awful', 188)\n",
      "('axioms', 189)\n",
      "('azaleas', 190)\n",
      "('back', 191)\n",
      "('background', 192)\n",
      "('balance', 193)\n",
      "('balancing', 194)\n",
      "('balustraded', 195)\n",
      "('basking', 196)\n",
      "('bath-rooms', 197)\n",
      "('be', 198)\n",
      "('beaming', 199)\n",
      "('bean-stalk', 200)\n",
      "('bear', 201)\n",
      "('beard', 202)\n",
      "('beauty', 203)\n",
      "('became', 204)\n",
      "('because', 205)\n",
      "('becoming', 206)\n",
      "('bed', 207)\n",
      "('been', 208)\n",
      "('before', 209)\n",
      "('began', 210)\n",
      "('begun', 211)\n",
      "('behind', 212)\n",
      "('being', 213)\n",
      "('believed', 214)\n",
      "('beneath', 215)\n",
      "('bespoke', 216)\n",
      "('better', 217)\n",
      "('between', 218)\n",
      "('big', 219)\n",
      "('bits', 220)\n",
      "('bitterness', 221)\n",
      "('blocked', 222)\n",
      "('born', 223)\n",
      "('borne', 224)\n",
      "('boudoir', 225)\n",
      "('bravura', 226)\n",
      "('break', 227)\n",
      "('breaking', 228)\n",
      "('breathing', 229)\n",
      "('bric-a-brac', 230)\n",
      "('briefly', 231)\n",
      "('brings', 232)\n",
      "('bronzes', 233)\n",
      "('brought', 234)\n",
      "('brown', 235)\n",
      "('brush', 236)\n",
      "('bull', 237)\n",
      "('business', 238)\n",
      "('but', 239)\n",
      "('buying', 240)\n",
      "('by', 241)\n",
      "('called', 242)\n",
      "('came', 243)\n",
      "('can', 244)\n",
      "('canvas', 245)\n",
      "('canvases', 246)\n",
      "('cards', 247)\n",
      "('care', 248)\n",
      "('career', 249)\n",
      "('caught', 250)\n",
      "('central', 251)\n",
      "('chair', 252)\n",
      "('chap', 253)\n",
      "('characteristic', 254)\n",
      "('charming', 255)\n",
      "('cheap', 256)\n",
      "('check', 257)\n",
      "('cheeks', 258)\n",
      "('chest', 259)\n",
      "('chimney-piece', 260)\n",
      "('chucked', 261)\n",
      "('cigar', 262)\n",
      "('cigarette', 263)\n",
      "('cigars', 264)\n",
      "('circulation', 265)\n",
      "('circumstance', 266)\n",
      "('circus-clown', 267)\n",
      "('claimed', 268)\n",
      "('clasping', 269)\n",
      "('clear', 270)\n",
      "('cleverer', 271)\n",
      "('close', 272)\n",
      "('clue', 273)\n",
      "('coat', 274)\n",
      "('collapsed', 275)\n",
      "('colour', 276)\n",
      "('come', 277)\n",
      "('comfortable', 278)\n",
      "('coming', 279)\n",
      "('companion', 280)\n",
      "('compared', 281)\n",
      "('complex', 282)\n",
      "('confident', 283)\n",
      "('congesting', 284)\n",
      "('conjugal', 285)\n",
      "('constraint', 286)\n",
      "('consummate', 287)\n",
      "('contended', 288)\n",
      "('continued', 289)\n",
      "('corner', 290)\n",
      "('corrected', 291)\n",
      "('could', 292)\n",
      "('couldn', 293)\n",
      "('count', 294)\n",
      "('countenance', 295)\n",
      "('couple', 296)\n",
      "('course', 297)\n",
      "('covered', 298)\n",
      "('craft', 299)\n",
      "('cried', 300)\n",
      "('crossed', 301)\n",
      "('crowned', 302)\n",
      "('crumbled', 303)\n",
      "('cry', 304)\n",
      "('cured', 305)\n",
      "('curiosity', 306)\n",
      "('curious', 307)\n",
      "('current', 308)\n",
      "('curtains', 309)\n",
      "('d', 310)\n",
      "('dabble', 311)\n",
      "('damask', 312)\n",
      "('dark', 313)\n",
      "('dashed', 314)\n",
      "('day', 315)\n",
      "('days', 316)\n",
      "('dead', 317)\n",
      "('deadening', 318)\n",
      "('dear', 319)\n",
      "('deep', 320)\n",
      "('deerhound', 321)\n",
      "('degree', 322)\n",
      "('delicate', 323)\n",
      "('demand', 324)\n",
      "('denied', 325)\n",
      "('deploring', 326)\n",
      "('deprecating', 327)\n",
      "('deprecatingly', 328)\n",
      "('desire', 329)\n",
      "('destroyed', 330)\n",
      "('destruction', 331)\n",
      "('desultory', 332)\n",
      "('detail', 333)\n",
      "('diagnosis', 334)\n",
      "('did', 335)\n",
      "('didn', 336)\n",
      "('died', 337)\n",
      "('dim', 338)\n",
      "('dimmest', 339)\n",
      "('dingy', 340)\n",
      "('dining-room', 341)\n",
      "('disarming', 342)\n",
      "('discovery', 343)\n",
      "('discrimination', 344)\n",
      "('discussion', 345)\n",
      "('disdain', 346)\n",
      "('disdained', 347)\n",
      "('disease', 348)\n",
      "('disguised', 349)\n",
      "('display', 350)\n",
      "('dissatisfied', 351)\n",
      "('distinguished', 352)\n",
      "('distract', 353)\n",
      "('divert', 354)\n",
      "('do', 355)\n",
      "('doesn', 356)\n",
      "('doing', 357)\n",
      "('domestic', 358)\n",
      "('don', 359)\n",
      "('done', 360)\n",
      "('donkey', 361)\n",
      "('down', 362)\n",
      "('dozen', 363)\n",
      "('dragged', 364)\n",
      "('drawing-room', 365)\n",
      "('drawing-rooms', 366)\n",
      "('drawn', 367)\n",
      "('dress-closets', 368)\n",
      "('drew', 369)\n",
      "('dropped', 370)\n",
      "('each', 371)\n",
      "('earth', 372)\n",
      "('ease', 373)\n",
      "('easel', 374)\n",
      "('easy', 375)\n",
      "('echoed', 376)\n",
      "('economy', 377)\n",
      "('effect', 378)\n",
      "('effects', 379)\n",
      "('efforts', 380)\n",
      "('egregious', 381)\n",
      "('eighteenth-century', 382)\n",
      "('elbow', 383)\n",
      "('elegant', 384)\n",
      "('else', 385)\n",
      "('embarrassed', 386)\n",
      "('enabled', 387)\n",
      "('end', 388)\n",
      "('endless', 389)\n",
      "('enjoy', 390)\n",
      "('enlightenment', 391)\n",
      "('enough', 392)\n",
      "('ensuing', 393)\n",
      "('equally', 394)\n",
      "('equanimity', 395)\n",
      "('escape', 396)\n",
      "('established', 397)\n",
      "('etching', 398)\n",
      "('even', 399)\n",
      "('event', 400)\n",
      "('ever', 401)\n",
      "('everlasting', 402)\n",
      "('every', 403)\n",
      "('exasperated', 404)\n",
      "('except', 405)\n",
      "('excuse', 406)\n",
      "('excusing', 407)\n",
      "('existed', 408)\n",
      "('expected', 409)\n",
      "('exquisite', 410)\n",
      "('exquisitely', 411)\n",
      "('extenuation', 412)\n",
      "('exterminating', 413)\n",
      "('extracting', 414)\n",
      "('eye', 415)\n",
      "('eyebrows', 416)\n",
      "('eyes', 417)\n",
      "('face', 418)\n",
      "('faces', 419)\n",
      "('fact', 420)\n",
      "('faded', 421)\n",
      "('failed', 422)\n",
      "('failure', 423)\n",
      "('fair', 424)\n",
      "('faith', 425)\n",
      "('false', 426)\n",
      "('familiar', 427)\n",
      "('famille-verte', 428)\n",
      "('fancy', 429)\n",
      "('fashionable', 430)\n",
      "('fate', 431)\n",
      "('feather', 432)\n",
      "('feet', 433)\n",
      "('fell', 434)\n",
      "('fellow', 435)\n",
      "('felt', 436)\n",
      "('few', 437)\n",
      "('fewer', 438)\n",
      "('finality', 439)\n",
      "('find', 440)\n",
      "('fingers', 441)\n",
      "('first', 442)\n",
      "('fit', 443)\n",
      "('fitting', 444)\n",
      "('five', 445)\n",
      "('flash', 446)\n",
      "('flashed', 447)\n",
      "('florid', 448)\n",
      "('flowers', 449)\n",
      "('fluently', 450)\n",
      "('flung', 451)\n",
      "('follow', 452)\n",
      "('followed', 453)\n",
      "('fond', 454)\n",
      "('footstep', 455)\n",
      "('for', 456)\n",
      "('forced', 457)\n",
      "('forcing', 458)\n",
      "('forehead', 459)\n",
      "('foreign', 460)\n",
      "('foreseen', 461)\n",
      "('forgive', 462)\n",
      "('forgotten', 463)\n",
      "('form', 464)\n",
      "('formed', 465)\n",
      "('forming', 466)\n",
      "('forward', 467)\n",
      "('fostered', 468)\n",
      "('found', 469)\n",
      "('foundations', 470)\n",
      "('fragment', 471)\n",
      "('fragments', 472)\n",
      "('frame', 473)\n",
      "('frames', 474)\n",
      "('frequently', 475)\n",
      "('friend', 476)\n",
      "('from', 477)\n",
      "('full', 478)\n",
      "('fullest', 479)\n",
      "('furiously', 480)\n",
      "('furrowed', 481)\n",
      "('garlanded', 482)\n",
      "('garlands', 483)\n",
      "('gave', 484)\n",
      "('genial', 485)\n",
      "('genius', 486)\n",
      "('gesture', 487)\n",
      "('get', 488)\n",
      "('getting', 489)\n",
      "('give', 490)\n",
      "('given', 491)\n",
      "('glad', 492)\n",
      "('glanced', 493)\n",
      "('glimpse', 494)\n",
      "('gloried', 495)\n",
      "('glory', 496)\n",
      "('go', 497)\n",
      "('going', 498)\n",
      "('gone', 499)\n",
      "('good', 500)\n",
      "('good-breeding', 501)\n",
      "('good-humoured', 502)\n",
      "('got', 503)\n",
      "('grace', 504)\n",
      "('gradually', 505)\n",
      "('gray', 506)\n",
      "('grayish', 507)\n",
      "('great', 508)\n",
      "('greatest', 509)\n",
      "('greatness', 510)\n",
      "('grew', 511)\n",
      "('groping', 512)\n",
      "('growing', 513)\n",
      "('had', 514)\n",
      "('hadn', 515)\n",
      "('hair', 516)\n",
      "('half', 517)\n",
      "('half-light', 518)\n",
      "('half-mechanically', 519)\n",
      "('hall', 520)\n",
      "('hand', 521)\n",
      "('hands', 522)\n",
      "('handsome', 523)\n",
      "('hanging', 524)\n",
      "('happen', 525)\n",
      "('happened', 526)\n",
      "('hard', 527)\n",
      "('hardly', 528)\n",
      "('has', 529)\n",
      "('have', 530)\n",
      "('haven', 531)\n",
      "('having', 532)\n",
      "('he', 533)\n",
      "('head', 534)\n",
      "('hear', 535)\n",
      "('heard', 536)\n",
      "('heart', 537)\n",
      "('height', 538)\n",
      "('her', 539)\n",
      "('here', 540)\n",
      "('hermit', 541)\n",
      "('herself', 542)\n",
      "('hesitations', 543)\n",
      "('hide', 544)\n",
      "('high', 545)\n",
      "('him', 546)\n",
      "('himself', 547)\n",
      "('hint', 548)\n",
      "('his', 549)\n",
      "('history', 550)\n",
      "('holding', 551)\n",
      "('home', 552)\n",
      "('honour', 553)\n",
      "('hooded', 554)\n",
      "('hostess', 555)\n",
      "('hot-house', 556)\n",
      "('hour', 557)\n",
      "('hours', 558)\n",
      "('house', 559)\n",
      "('how', 560)\n",
      "('hung', 561)\n",
      "('husband', 562)\n",
      "('idea', 563)\n",
      "('idle', 564)\n",
      "('idling', 565)\n",
      "('if', 566)\n",
      "('immediately', 567)\n",
      "('in', 568)\n",
      "('incense', 569)\n",
      "('indifferent', 570)\n",
      "('inevitable', 571)\n",
      "('inevitably', 572)\n",
      "('inflexible', 573)\n",
      "('insensible', 574)\n",
      "('insignificant', 575)\n",
      "('instinctively', 576)\n",
      "('instructive', 577)\n",
      "('interesting', 578)\n",
      "('into', 579)\n",
      "('ironic', 580)\n",
      "('irony', 581)\n",
      "('irrelevance', 582)\n",
      "('irrevocable', 583)\n",
      "('is', 584)\n",
      "('it', 585)\n",
      "('its', 586)\n",
      "('itself', 587)\n",
      "('jardiniere', 588)\n",
      "('jealousy', 589)\n",
      "('just', 590)\n",
      "('keep', 591)\n",
      "('kept', 592)\n",
      "('kind', 593)\n",
      "('knees', 594)\n",
      "('knew', 595)\n",
      "('know', 596)\n",
      "('known', 597)\n",
      "('laid', 598)\n",
      "('lair', 599)\n",
      "('landing', 600)\n",
      "('language', 601)\n",
      "('last', 602)\n",
      "('late', 603)\n",
      "('later', 604)\n",
      "('latter', 605)\n",
      "('laugh', 606)\n",
      "('laughed', 607)\n",
      "('lay', 608)\n",
      "('leading', 609)\n",
      "('lean', 610)\n",
      "('learned', 611)\n",
      "('least', 612)\n",
      "('leathery', 613)\n",
      "('leave', 614)\n",
      "('led', 615)\n",
      "('left', 616)\n",
      "('leisure', 617)\n",
      "('lends', 618)\n",
      "('lent', 619)\n",
      "('let', 620)\n",
      "('lies', 621)\n",
      "('life', 622)\n",
      "('life-likeness', 623)\n",
      "('lift', 624)\n",
      "('lifted', 625)\n",
      "('light', 626)\n",
      "('lightly', 627)\n",
      "('like', 628)\n",
      "('liked', 629)\n",
      "('line', 630)\n",
      "('lines', 631)\n",
      "('lingered', 632)\n",
      "('lips', 633)\n",
      "('lit', 634)\n",
      "('little', 635)\n",
      "('live', 636)\n",
      "('ll', 637)\n",
      "('loathing', 638)\n",
      "('long', 639)\n",
      "('longed', 640)\n",
      "('longer', 641)\n",
      "('look', 642)\n",
      "('looked', 643)\n",
      "('looking', 644)\n",
      "('lose', 645)\n",
      "('loss', 646)\n",
      "('lounging', 647)\n",
      "('lovely', 648)\n",
      "('lucky', 649)\n",
      "('lump', 650)\n",
      "('luncheon-table', 651)\n",
      "('luxury', 652)\n",
      "('lying', 653)\n",
      "('made', 654)\n",
      "('make', 655)\n",
      "('man', 656)\n",
      "('manage', 657)\n",
      "('managed', 658)\n",
      "('mantel-piece', 659)\n",
      "('marble', 660)\n",
      "('married', 661)\n",
      "('may', 662)\n",
      "('me', 663)\n",
      "('meant', 664)\n",
      "('mediocrity', 665)\n",
      "('medium', 666)\n",
      "('mentioned', 667)\n",
      "('mere', 668)\n",
      "('merely', 669)\n",
      "('met', 670)\n",
      "('might', 671)\n",
      "('mighty', 672)\n",
      "('millionaire', 673)\n",
      "('mine', 674)\n",
      "('minute', 675)\n",
      "('minutes', 676)\n",
      "('mirrors', 677)\n",
      "('modest', 678)\n",
      "('modesty', 679)\n",
      "('moment', 680)\n",
      "('money', 681)\n",
      "('monumental', 682)\n",
      "('mood', 683)\n",
      "('morbidly', 684)\n",
      "('more', 685)\n",
      "('most', 686)\n",
      "('mourn', 687)\n",
      "('mourned', 688)\n",
      "('moustache', 689)\n",
      "('moved', 690)\n",
      "('much', 691)\n",
      "('muddling', 692)\n",
      "('multiplied', 693)\n",
      "('murmur', 694)\n",
      "('muscles', 695)\n",
      "('must', 696)\n",
      "('my', 697)\n",
      "('myself', 698)\n",
      "('mysterious', 699)\n",
      "('naive', 700)\n",
      "('near', 701)\n",
      "('nearly', 702)\n",
      "('negatived', 703)\n",
      "('nervous', 704)\n",
      "('nervousness', 705)\n",
      "('neutral', 706)\n",
      "('never', 707)\n",
      "('next', 708)\n",
      "('no', 709)\n",
      "('none', 710)\n",
      "('not', 711)\n",
      "('note', 712)\n",
      "('nothing', 713)\n",
      "('now', 714)\n",
      "('nymphs', 715)\n",
      "('oak', 716)\n",
      "('obituary', 717)\n",
      "('object', 718)\n",
      "('objects', 719)\n",
      "('occurred', 720)\n",
      "('oddly', 721)\n",
      "('of', 722)\n",
      "('off', 723)\n",
      "('often', 724)\n",
      "('oh', 725)\n",
      "('old', 726)\n",
      "('on', 727)\n",
      "('once', 728)\n",
      "('one', 729)\n",
      "('ones', 730)\n",
      "('only', 731)\n",
      "('onto', 732)\n",
      "('open', 733)\n",
      "('or', 734)\n",
      "('other', 735)\n",
      "('our', 736)\n",
      "('ourselves', 737)\n",
      "('out', 738)\n",
      "('outline', 739)\n",
      "('oval', 740)\n",
      "('over', 741)\n",
      "('own', 742)\n",
      "('packed', 743)\n",
      "('paid', 744)\n",
      "('paint', 745)\n",
      "('painted', 746)\n",
      "('painter', 747)\n",
      "('painting', 748)\n",
      "('pale', 749)\n",
      "('paled', 750)\n",
      "('palm-trees', 751)\n",
      "('panel', 752)\n",
      "('panelling', 753)\n",
      "('pardonable', 754)\n",
      "('pardoned', 755)\n",
      "('part', 756)\n",
      "('passages', 757)\n",
      "('passing', 758)\n",
      "('past', 759)\n",
      "('pastels', 760)\n",
      "('pathos', 761)\n",
      "('patient', 762)\n",
      "('people', 763)\n",
      "('perceptible', 764)\n",
      "('perfect', 765)\n",
      "('persistence', 766)\n",
      "('persuasively', 767)\n",
      "('phrase', 768)\n",
      "('picture', 769)\n",
      "('pictures', 770)\n",
      "('pines', 771)\n",
      "('pink', 772)\n",
      "('place', 773)\n",
      "('placed', 774)\n",
      "('plain', 775)\n",
      "('platitudes', 776)\n",
      "('pleased', 777)\n",
      "('pockets', 778)\n",
      "('point', 779)\n",
      "('poised', 780)\n",
      "('poor', 781)\n",
      "('portrait', 782)\n",
      "('posing', 783)\n",
      "('possessed', 784)\n",
      "('poverty', 785)\n",
      "('predicted', 786)\n",
      "('preliminary', 787)\n",
      "('presenting', 788)\n",
      "('prestidigitation', 789)\n",
      "('pretty', 790)\n",
      "('previous', 791)\n",
      "('price', 792)\n",
      "('pride', 793)\n",
      "('princely', 794)\n",
      "('prism', 795)\n",
      "('problem', 796)\n",
      "('proclaiming', 797)\n",
      "('prodigious', 798)\n",
      "('profusion', 799)\n",
      "('protest', 800)\n",
      "('prove', 801)\n",
      "('public', 802)\n",
      "('purblind', 803)\n",
      "('purely', 804)\n",
      "('pushed', 805)\n",
      "('put', 806)\n",
      "('qualities', 807)\n",
      "('quality', 808)\n",
      "('queerly', 809)\n",
      "('question', 810)\n",
      "('quickly', 811)\n",
      "('quietly', 812)\n",
      "('quite', 813)\n",
      "('quote', 814)\n",
      "('rain', 815)\n",
      "('raised', 816)\n",
      "('random', 817)\n",
      "('rather', 818)\n",
      "('re', 819)\n",
      "('real', 820)\n",
      "('really', 821)\n",
      "('reared', 822)\n",
      "('reason', 823)\n",
      "('reassurance', 824)\n",
      "('recovering', 825)\n",
      "('recreated', 826)\n",
      "('reflected', 827)\n",
      "('reflection', 828)\n",
      "('regrets', 829)\n",
      "('relatively', 830)\n",
      "('remained', 831)\n",
      "('remember', 832)\n",
      "('reminded', 833)\n",
      "('repeating', 834)\n",
      "('represented', 835)\n",
      "('reproduction', 836)\n",
      "('resented', 837)\n",
      "('resolve', 838)\n",
      "('resources', 839)\n",
      "('rest', 840)\n",
      "('rich', 841)\n",
      "('ridiculous', 842)\n",
      "('robbed', 843)\n",
      "('romantic', 844)\n",
      "('room', 845)\n",
      "('rose', 846)\n",
      "('rs', 847)\n",
      "('rule', 848)\n",
      "('run', 849)\n",
      "('s', 850)\n",
      "('said', 851)\n",
      "('same', 852)\n",
      "('satisfaction', 853)\n",
      "('savour', 854)\n",
      "('saw', 855)\n",
      "('say', 856)\n",
      "('saying', 857)\n",
      "('says', 858)\n",
      "('scorn', 859)\n",
      "('scornful', 860)\n",
      "('secret', 861)\n",
      "('see', 862)\n",
      "('seemed', 863)\n",
      "('seen', 864)\n",
      "('self-confident', 865)\n",
      "('send', 866)\n",
      "('sensation', 867)\n",
      "('sensitive', 868)\n",
      "('sent', 869)\n",
      "('serious', 870)\n",
      "('set', 871)\n",
      "('sex', 872)\n",
      "('shade', 873)\n",
      "('shaking', 874)\n",
      "('shall', 875)\n",
      "('she', 876)\n",
      "('shirked', 877)\n",
      "('short', 878)\n",
      "('should', 879)\n",
      "('shoulder', 880)\n",
      "('shoulders', 881)\n",
      "('show', 882)\n",
      "('showed', 883)\n",
      "('showy', 884)\n",
      "('shrug', 885)\n",
      "('shrugged', 886)\n",
      "('sight', 887)\n",
      "('sign', 888)\n",
      "('silent', 889)\n",
      "('silver', 890)\n",
      "('similar', 891)\n",
      "('simpleton', 892)\n",
      "('simplifications', 893)\n",
      "('simply', 894)\n",
      "('since', 895)\n",
      "('single', 896)\n",
      "('sitter', 897)\n",
      "('sitters', 898)\n",
      "('sketch', 899)\n",
      "('skill', 900)\n",
      "('slight', 901)\n",
      "('slightly', 902)\n",
      "('slowly', 903)\n",
      "('small', 904)\n",
      "('smile', 905)\n",
      "('smiling', 906)\n",
      "('sneer', 907)\n",
      "('so', 908)\n",
      "('solace', 909)\n",
      "('some', 910)\n",
      "('somebody', 911)\n",
      "('something', 912)\n",
      "('spacious', 913)\n",
      "('spaniel', 914)\n",
      "('speaking-tubes', 915)\n",
      "('speculations', 916)\n",
      "('spite', 917)\n",
      "('splash', 918)\n",
      "('square', 919)\n",
      "('stairs', 920)\n",
      "('stammer', 921)\n",
      "('stand', 922)\n",
      "('standing', 923)\n",
      "('started', 924)\n",
      "('stay', 925)\n",
      "('still', 926)\n",
      "('stocked', 927)\n",
      "('stood', 928)\n",
      "('stopped', 929)\n",
      "('stopping', 930)\n",
      "('straddling', 931)\n",
      "('straight', 932)\n",
      "('strain', 933)\n",
      "('straining', 934)\n",
      "('strange', 935)\n",
      "('straw', 936)\n",
      "('stream', 937)\n",
      "('stroke', 938)\n",
      "('strokes', 939)\n",
      "('strolled', 940)\n",
      "('strongest', 941)\n",
      "('strongly', 942)\n",
      "('struck', 943)\n",
      "('studio', 944)\n",
      "('stuff', 945)\n",
      "('subject', 946)\n",
      "('substantial', 947)\n",
      "('suburban', 948)\n",
      "('such', 949)\n",
      "('suddenly', 950)\n",
      "('suffered', 951)\n",
      "('sugar', 952)\n",
      "('suggested', 953)\n",
      "('sunburn', 954)\n",
      "('sunburnt', 955)\n",
      "('sunlit', 956)\n",
      "('superb', 957)\n",
      "('sure', 958)\n",
      "('surest', 959)\n",
      "('surface', 960)\n",
      "('surprise', 961)\n",
      "('surprised', 962)\n",
      "('surrounded', 963)\n",
      "('suspected', 964)\n",
      "('sweetly', 965)\n",
      "('sweetness', 966)\n",
      "('swelling', 967)\n",
      "('swept', 968)\n",
      "('swum', 969)\n",
      "('t', 970)\n",
      "('table', 971)\n",
      "('take', 972)\n",
      "('taken', 973)\n",
      "('talking', 974)\n",
      "('tea', 975)\n",
      "('tears', 976)\n",
      "('technicalities', 977)\n",
      "('technique', 978)\n",
      "('tell', 979)\n",
      "('tells', 980)\n",
      "('tempting', 981)\n",
      "('terra-cotta', 982)\n",
      "('terrace', 983)\n",
      "('terraces', 984)\n",
      "('terribly', 985)\n",
      "('than', 986)\n",
      "('that', 987)\n",
      "('the', 988)\n",
      "('their', 989)\n",
      "('them', 990)\n",
      "('then', 991)\n",
      "('there', 992)\n",
      "('therefore', 993)\n",
      "('they', 994)\n",
      "('thin', 995)\n",
      "('thing', 996)\n",
      "('things', 997)\n",
      "('think', 998)\n",
      "('this', 999)\n",
      "('thither', 1000)\n",
      "('those', 1001)\n",
      "('though', 1002)\n",
      "('thought', 1003)\n",
      "('three', 1004)\n",
      "('threshold', 1005)\n",
      "('threw', 1006)\n",
      "('through', 1007)\n",
      "('throwing', 1008)\n",
      "('tie', 1009)\n",
      "('till', 1010)\n",
      "('time', 1011)\n",
      "('timorously', 1012)\n",
      "('tinge', 1013)\n",
      "('tips', 1014)\n",
      "('tired', 1015)\n",
      "('to', 1016)\n",
      "('told', 1017)\n",
      "('tone', 1018)\n",
      "('tones', 1019)\n",
      "('too', 1020)\n",
      "('took', 1021)\n",
      "('tottering', 1022)\n",
      "('touched', 1023)\n",
      "('toward', 1024)\n",
      "('trace', 1025)\n",
      "('trade', 1026)\n",
      "('transmute', 1027)\n",
      "('traps', 1028)\n",
      "('travelled', 1029)\n",
      "('tribute', 1030)\n",
      "('tributes', 1031)\n",
      "('tricks', 1032)\n",
      "('tried', 1033)\n",
      "('trouser-presses', 1034)\n",
      "('true', 1035)\n",
      "('truth', 1036)\n",
      "('turned', 1037)\n",
      "('twenty', 1038)\n",
      "('twenty-four', 1039)\n",
      "('twice', 1040)\n",
      "('twirling', 1041)\n",
      "('unaccountable', 1042)\n",
      "('uncertain', 1043)\n",
      "('under', 1044)\n",
      "('underlay', 1045)\n",
      "('underneath', 1046)\n",
      "('understand', 1047)\n",
      "('unexpected', 1048)\n",
      "('untouched', 1049)\n",
      "('unusual', 1050)\n",
      "('up', 1051)\n",
      "('up-stream', 1052)\n",
      "('upon', 1053)\n",
      "('upset', 1054)\n",
      "('upstairs', 1055)\n",
      "('us', 1056)\n",
      "('used', 1057)\n",
      "('usual', 1058)\n",
      "('value', 1059)\n",
      "('varnishing', 1060)\n",
      "('vases', 1061)\n",
      "('ve', 1062)\n",
      "('veins', 1063)\n",
      "('velveteen', 1064)\n",
      "('very', 1065)\n",
      "('villa', 1066)\n",
      "('vindicated', 1067)\n",
      "('virtuosity', 1068)\n",
      "('vista', 1069)\n",
      "('vocation', 1070)\n",
      "('voice', 1071)\n",
      "('wall', 1072)\n",
      "('wander', 1073)\n",
      "('want', 1074)\n",
      "('wanted', 1075)\n",
      "('wants', 1076)\n",
      "('was', 1077)\n",
      "('wasn', 1078)\n",
      "('watched', 1079)\n",
      "('watching', 1080)\n",
      "('water-colour', 1081)\n",
      "('waves', 1082)\n",
      "('way', 1083)\n",
      "('weekly', 1084)\n",
      "('weeks', 1085)\n",
      "('welcome', 1086)\n",
      "('went', 1087)\n",
      "('were', 1088)\n",
      "('what', 1089)\n",
      "('when', 1090)\n",
      "('whenever', 1091)\n",
      "('where', 1092)\n",
      "('which', 1093)\n",
      "('while', 1094)\n",
      "('white', 1095)\n",
      "('white-panelled', 1096)\n",
      "('who', 1097)\n",
      "('whole', 1098)\n",
      "('whom', 1099)\n",
      "('why', 1100)\n",
      "('wide', 1101)\n",
      "('widow', 1102)\n",
      "('wife', 1103)\n",
      "('wild', 1104)\n",
      "('wincing', 1105)\n",
      "('window-curtains', 1106)\n",
      "('wish', 1107)\n",
      "('with', 1108)\n",
      "('without', 1109)\n",
      "('wits', 1110)\n",
      "('woman', 1111)\n",
      "('women', 1112)\n",
      "('won', 1113)\n",
      "('wonder', 1114)\n",
      "('wondered', 1115)\n",
      "('word', 1116)\n",
      "('work', 1117)\n",
      "('working', 1118)\n",
      "('worth', 1119)\n",
      "('would', 1120)\n",
      "('wouldn', 1121)\n",
      "('year', 1122)\n",
      "('years', 1123)\n",
      "('yellow', 1124)\n",
      "('yet', 1125)\n",
      "('you', 1126)\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e537d324-36e8-4d80-97d8-a651c9cc0ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131]\n"
     ]
    }
   ],
   "source": [
    "ids = [vocab[s] for s in vocab]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "09a67895-cea7-40d1-ad05-f36c29dbd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! \" ' ( ) , -- . : ; ? A Ah Among And Are Arrt As At Be Begin Burlington But By Carlo Chicago Claude Come Croft Destroyed Devonshire Don Dubarry Emperors Florence For Gallery Gideon Gisburn Gisburns Grafton Greek Grindle Grindles HAD Had Hang Has He Her Hermia His How I If In It Jack Jove Just Lord Made Miss Money Monte Moon-dancers Mr Mrs My Never No Now Nutley Of Oh On Once Only Or Perhaps Poor Professional Renaissance Rickham Riviera Rome Russian Sevres She Stroud Strouds Suddenly That The Then There They This Those Though Thwing Thwings To Usually Venetian Victor Was We Well What When Why Yes You _ a abdication able about above abruptly absolute absorbed absurdity academic accuse accustomed across activity add added admirers adopted adulation advance aesthetic affect afraid after afterward again ago ah air alive all almost alone along always am amazement amid among amplest amusing an and another answer answered any anything anywhere apparent apparently appearance appeared appointed are arm arm-chair arm-chairs arms art articles artist as aside asked at atmosphere atom attack attention attitude audacities away awful axioms azaleas back background balance balancing balustraded basking bath-rooms be beaming bean-stalk bear beard beauty became because becoming bed been before began begun behind being believed beneath bespoke better between big bits bitterness blocked born borne boudoir bravura break breaking breathing bric-a-brac briefly brings bronzes brought brown brush bull business but buying by called came can canvas canvases cards care career caught central chair chap characteristic charming cheap check cheeks chest chimney-piece chucked cigar cigarette cigars circulation circumstance circus-clown claimed clasping clear cleverer close clue coat collapsed colour come comfortable coming companion compared complex confident congesting conjugal constraint consummate contended continued corner corrected could couldn count countenance couple course covered craft cried crossed crowned crumbled cry cured curiosity curious current curtains d dabble damask dark dashed day days dead deadening dear deep deerhound degree delicate demand denied deploring deprecating deprecatingly desire destroyed destruction desultory detail diagnosis did didn died dim dimmest dingy dining-room disarming discovery discrimination discussion disdain disdained disease disguised display dissatisfied distinguished distract divert do doesn doing domestic don done donkey down dozen dragged drawing-room drawing-rooms drawn dress-closets drew dropped each earth ease easel easy echoed economy effect effects efforts egregious eighteenth-century elbow elegant else embarrassed enabled end endless enjoy enlightenment enough ensuing equally equanimity escape established etching even event ever everlasting every exasperated except excuse excusing existed expected exquisite exquisitely extenuation exterminating extracting eye eyebrows eyes face faces fact faded failed failure fair faith false familiar famille-verte fancy fashionable fate feather feet fell fellow felt few fewer finality find fingers first fit fitting five flash flashed florid flowers fluently flung follow followed fond footstep for forced forcing forehead foreign foreseen forgive forgotten form formed forming forward fostered found foundations fragment fragments frame frames frequently friend from full fullest furiously furrowed garlanded garlands gave genial genius gesture get getting give given glad glanced glimpse gloried glory go going gone good good-breeding good-humoured got grace gradually gray grayish great greatest greatness grew groping growing had hadn hair half half-light half-mechanically hall hand hands handsome hanging happen happened hard hardly has have haven having he head hear heard heart height her here hermit herself hesitations hide high him himself hint his history holding home honour hooded hostess hot-house hour hours house how hung husband idea idle idling if immediately in incense indifferent inevitable inevitably inflexible insensible insignificant instinctively instructive interesting into ironic irony irrelevance irrevocable is it its itself jardiniere jealousy just keep kept kind knees knew know known laid lair landing language last late later latter laugh laughed lay leading lean learned least leathery leave led left leisure lends lent let lies life life-likeness lift lifted light lightly like liked line lines lingered lips lit little live ll loathing long longed longer look looked looking lose loss lounging lovely lucky lump luncheon-table luxury lying made make man manage managed mantel-piece marble married may me meant mediocrity medium mentioned mere merely met might mighty millionaire mine minute minutes mirrors modest modesty moment money monumental mood morbidly more most mourn mourned moustache moved much muddling multiplied murmur muscles must my myself mysterious naive near nearly negatived nervous nervousness neutral never next no none not note nothing now nymphs oak obituary object objects occurred oddly of off often oh old on once one ones only onto open or other our ourselves out outline oval over own packed paid paint painted painter painting pale paled palm-trees panel panelling pardonable pardoned part passages passing past pastels pathos patient people perceptible perfect persistence persuasively phrase picture pictures pines pink place placed plain platitudes pleased pockets point poised poor portrait posing possessed poverty predicted preliminary presenting prestidigitation pretty previous price pride princely prism problem proclaiming prodigious profusion protest prove public purblind purely pushed put qualities quality queerly question quickly quietly quite quote rain raised random rather re real really reared reason reassurance recovering recreated reflected reflection regrets relatively remained remember reminded repeating represented reproduction resented resolve resources rest rich ridiculous robbed romantic room rose rs rule run s said same satisfaction savour saw say saying says scorn scornful secret see seemed seen self-confident send sensation sensitive sent serious set sex shade shaking shall she shirked short should shoulder shoulders show showed showy shrug shrugged sight sign silent silver similar simpleton simplifications simply since single sitter sitters sketch skill slight slightly slowly small smile smiling sneer so solace some somebody something spacious spaniel speaking-tubes speculations spite splash square stairs stammer stand standing started stay still stocked stood stopped stopping straddling straight strain straining strange straw stream stroke strokes strolled strongest strongly struck studio stuff subject substantial suburban such suddenly suffered sugar suggested sunburn sunburnt sunlit superb sure surest surface surprise surprised surrounded suspected sweetly sweetness swelling swept swum t table take taken talking tea tears technicalities technique tell tells tempting terra-cotta terrace terraces terribly than that the their them then there therefore they thin thing things think this thither those though thought three threshold threw through throwing tie till time timorously tinge tips tired to told tone tones too took tottering touched toward trace trade transmute traps travelled tribute tributes tricks tried trouser-presses true truth turned twenty twenty-four twice twirling unaccountable uncertain under underlay underneath understand unexpected untouched unusual up up-stream upon upset upstairs us used usual value varnishing vases ve veins velveteen very villa vindicated virtuosity vista vocation voice wall wander want wanted wants was wasn watched watching water-colour waves way weekly weeks welcome went were what when whenever where which while white white-panelled who whole whom why wide widow wife wild wincing window-curtains wish with without wits woman women won wonder wondered word work working worth would wouldn year years yellow yet you younger your yourself <|endoftext|> <|unk|>\n"
     ]
    }
   ],
   "source": [
    "id_to_token = {integer: token for token, integer in vocab.items()}\n",
    "integer_to_string = [id_to_token[i] for i in ids]\n",
    "text = \" \".join(integer_to_string)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "582bbcfc-96d0-4a36-a821-4ec2fa8f77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)                  \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0345c68-922f-4e6d-ae20-e4f9ce9918e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129, '<|endoftext|>': 1130, '<|unk|>': 1131}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aae24c7b-7b39-49ed-bde8-b7cd628643bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"wish with without wits woman women won wonder wondered word work working worth would wouldn year years yellow yet you younger your yourself\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca9d286c-d9d6-49e9-a9d7-a41479807176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wish with without wits woman women won wonder wondered word work working worth would wouldn year years yellow yet you younger your yourself\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a3a1b18-887a-4e70-b8d4-b850e0f27ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"Hello wish with without wits woman women won wonder wondered word work working worth would wouldn year years yellow yet you younger your yourself\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e144da47-4b71-413d-a55b-ce396b83275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897, 6, 326, 549, 1042, 116, 7, 1, 73, 297, 585, 2, 850, 498, 1016, 866, 988, 1059, 722, 697, 769, 2, 1083, 1051, 9, 239, 53, 359, 2, 970, 998, 722, 987, 5, 66, 7, 83, 6, 988, 646, 1016, 16, 584, 145, 53, 998, 722, 7, 1, 93, 1116, 5, 727, 67, 7, 100, 2, 850, 633, 5, 693, 586, 114, 847, 114, 177, 1002, 994, 1088, 827, 568, 156, 389, 1069, 722, 677, 7, 14, 585, 1077, 711, 731, 988, 67, 7, 101, 1097, 688, 7, 45, 711, 988, 410, 50, 28, 5, 180, 988, 602, 40, 36, 882, 5, 929, 663, 209, 38, 2, 850, 1, 65, 1, 1016, 856, 5, 1108, 976, 568, 539, 417, 8, 1, 107, 875, 711, 642, 1053, 586, 628, 140, 1, 10, 108, 0, 6, 399, 1007, 988, 795, 722, 50, 2, 850, 976, 53, 436, 117, 1016, 418, 988, 420, 1108, 395, 7, 80, 57, 38, 0, 93, 1112, 514, 654, 546, 6, 585, 1077, 444, 987, 994, 879, 687, 546, 7, 13, 549, 742, 872, 438, 829, 1088, 536, 5, 157, 568, 549, 742, 1026, 528, 115, 694, 7, 81, 589, 10, 79, 7, 54, 585, 1088, 5, 988, 553, 722, 988, 299, 1077, 1067, 241, 635, 26, 72, 5, 1097, 5, 568, 145, 500, 425, 5, 234, 738, 568, 988, 21, 115, 1065, 523, 1, 717, 1, 727, 57, 6, 729, 722, 1001, 884, 175, 927, 1108, 817, 977, 987, 53, 530, 536, 3, 53, 1113, 2, 970, 856, 241, 1099, 4, 281, 1016, 38, 2, 850, 748, 7, 14, 908, 6, 549, 838, 213, 165, 583, 6, 988, 345, 505, 337, 738, 5, 157, 5, 177, 67, 7, 100, 514, 786, 5, 988, 792, 722, 1, 39, 1, 1087, 1051, 7, 56, 1077, 711, 1010, 1004, 1123, 604, 987, 5, 568, 988, 297, 722, 115, 437, 1085, 2, 565, 727, 988, 84, 5, 585, 950, 720, 1016, 663, 1016, 1114, 1100, 38, 514, 491, 1051, 549, 748, 7, 75, 828, 5, 585, 821, 1077, 115, 981, 796, 7, 102, 125, 549, 1103, 1120, 530, 208, 1020, 375, 6, 549, 424, 898, 514, 208, 325, 988, 909, 722, 857, 987, 67, 7, 38, 514, 1, 364, 546, 362, 7, 1, 35, 67, 7, 38, 6, 177, 949, 6, 514, 711, 408, 1010, 702, 115, 1122, 138, 57, 2, 850, 838, 514, 208, 973, 7, 56, 671, 198, 987, 533, 514, 661, 539, 6, 895, 533, 629, 549, 373, 6, 205, 533, 336, 2, 970, 1074, 1016, 497, 727, 748, 9, 239, 585, 1120, 530, 208, 527, 1016, 801, 987, 533, 514, 491, 1051, 549, 748, 205, 533, 514, 661, 539, 7, 73, 297, 5, 566, 876, 514, 711, 364, 546, 362, 5, 876, 514, 394, 5, 177, 62, 28, 288, 5, 422, 1016, 1, 624, 546, 1051, 1, 6, 876, 514, 711, 615, 546, 191, 1016, 988, 374, 7, 102, 806, 988, 236, 579, 549, 521, 140, 6, 1089, 115, 1070, 456, 115, 1103, 0, 22, 67, 7, 38, 167, 1016, 530, 347, 585, 6, 157, 53, 436, 585, 671, 198, 578, 1016, 440, 738, 1100, 7, 93, 332, 622, 722, 988, 84, 618, 587, 1016, 949, 804, 124, 916, 9, 157, 532, 5, 727, 697, 1083, 1016, 64, 24, 5, 250, 115, 494, 722, 57, 2, 850, 195, 984, 218, 988, 771, 5, 53, 514, 698, 224, 1000, 988, 708, 315, 7, 53, 469, 988, 296, 180, 975, 215, 989, 751, 9, 157, 67, 7, 38, 2, 850, 1086, 1077, 908, 485, 987, 5, 568, 988, 393, 1085, 5, 53, 268, 585, 475, 7, 56, 1077, 711, 987, 697, 555, 1077, 1, 578, 1, 8, 727, 987, 779, 53, 292, 530, 491, 62, 28, 988, 479, 824, 7, 56, 1077, 590, 205, 876, 1077, 114, 711, 114, 578, 6, 566, 53, 662, 198, 755, 988, 237, 6, 987, 53, 469, 539, 908, 7, 35, 57, 5, 145, 549, 622, 5, 514, 208, 963, 241, 578, 1112, 8, 994, 514, 468, 549, 174, 5, 585, 514, 208, 822, 568, 988, 556, 722, 989, 133, 7, 14, 585, 1077, 993, 577, 1016, 712, 1089, 378, 988, 1, 318, 181, 722, 665, 1, 3, 53, 814, 62, 28, 4, 1077, 532, 727, 546, 7, 53, 530, 667, 987, 67, 7, 38, 1077, 841, 9, 157, 585, 1077, 567, 764, 987, 539, 562, 1077, 414, 477, 999, 266, 115, 323, 239, 947, 853, 7, 56, 584, 5, 177, 115, 848, 5, 988, 763, 1097, 859, 681, 1097, 488, 686, 738, 722, 585, 9, 157, 57, 2, 850, 384, 346, 722, 549, 1103, 2, 850, 219, 193, 387, 546, 5, 1108, 156, 166, 722, 765, 501, 5, 1016, 1027, 585, 579, 719, 722, 174, 157, 652, 7, 102, 988, 605, 5, 53, 696, 129, 5, 533, 831, 830, 570, 9, 239, 533, 1077, 240, 82, 233, 157, 382, 770, 1108, 115, 344, 987, 216, 988, 154, 839, 7, 1, 63, 2, 850, 731, 406, 584, 1016, 806, 203, 579, 265, 5, 1, 1077, 729, 722, 988, 189, 533, 598, 362, 127, 988, 87, 157, 890, 722, 156, 411, 168, 651, 5, 1090, 5, 727, 115, 604, 315, 5, 53, 514, 140, 849, 741, 477, 64, 24, 9, 157, 67, 7, 38, 5, 199, 727, 546, 5, 130, 456, 697, 391, 8, 1, 57, 584, 908, 684, 868, 1016, 403, 464, 722, 203, 7, 1, 80, 57, 0, 56, 514, 149, 208, 549, 431, 1016, 530, 1112, 856, 949, 997, 722, 546, 8, 988, 420, 879, 198, 871, 362, 568, 412, 7, 109, 943, 663, 714, 1077, 987, 5, 456, 988, 442, 1011, 5, 533, 837, 988, 1018, 7, 53, 514, 864, 546, 5, 908, 724, 5, 196, 1044, 891, 1031, 6, 1077, 585, 988, 285, 712, 987, 843, 990, 722, 989, 854, 10, 70, 6, 456, 5, 721, 392, 5, 585, 204, 164, 987, 533, 1077, 454, 722, 67, 7, 38, 6, 454, 392, 711, 1016, 862, 539, 123, 7, 56, 1077, 549, 742, 123, 533, 863, 1016, 198, 1105, 1044, 6, 549, 742, 185, 177, 156, 718, 456, 483, 157, 569, 7, 1, 68, 319, 5, 895, 53, 2, 1062, 261, 748, 763, 359, 2, 970, 856, 987, 945, 118, 663, 6, 994, 856, 585, 118, 105, 42, 5, 1, 1077, 549, 731, 800, 5, 177, 533, 846, 477, 988, 971, 157, 940, 738, 732, 988, 956, 983, 7, 53, 493, 138, 546, 5, 943, 241, 549, 602, 1116, 7, 105, 42, 1077, 5, 568, 420, 5, 206, 988, 656, 722, 988, 680, 6, 177, 57, 547, 5, 729, 671, 806, 585, 5, 514, 208, 988, 656, 722, 988, 557, 7, 93, 1127, 176, 1077, 851, 1016, 530, 465, 547, 180, 697, 476, 2, 850, 433, 5, 157, 53, 1115, 566, 115, 1013, 722, 589, 1045, 988, 605, 2, 850, 699, 116, 7, 22, 709, 6, 456, 585, 1077, 711, 1010, 138, 987, 400, 987, 988, 114, 846, 32, 114, 366, 514, 211, 1016, 350, 989, 1, 43, 7, 1, 53, 1037, 1016, 67, 7, 38, 5, 1097, 514, 632, 1016, 490, 115, 650, 722, 952, 1016, 539, 914, 568, 988, 341, 7, 1, 111, 114, 529, 114, 533, 261, 748, 10, 1, 53, 179, 120, 7, 88, 816, 539, 416, 1108, 115, 548, 722, 502, 961, 7, 1, 74, 5, 533, 356, 2, 970, 114, 530, 114, 1016, 714, 5, 1126, 596, 9, 157, 53, 1074, 546, 1016, 390, 547, 5, 1, 876, 851, 813, 894, 7, 53, 643, 118, 988, 913, 1096, 845, 5, 1108, 586, 114, 428, 114, 1061, 834, 988, 1019, 722, 988, 749, 312, 309, 5, 157, 586, 382, 760, 568, 323, 421, 474, 7, 1, 47, 533, 261, 549, 770, 1020, 10, 53, 531, 2, 970, 864, 115, 896, 729, 568, 988, 559, 7, 1, 11, 901, 873, 722, 286, 301, 67, 7, 38, 2, 850, 733, 295, 7, 1, 56, 2, 850, 549, 842, 679, 5, 1126, 596, 7, 48, 858, 994, 2, 819, 711, 443, 1016, 530, 118, 9, 533, 2, 850, 869, 990, 145, 187, 405, 729, 6, 697, 782, 6, 157, 987, 53, 530, 1016, 591, 1055, 7, 1, 51, 842, 679, 6, 57, 2, 850, 679, 118, 549, 770, 10, 68, 306, 1077, 513, 628, 988, 200, 7, 53, 851, 767, 1016, 697, 555, 8, 1, 53, 696, 821, 862, 1128, 782, 5, 1126, 596, 7, 1, 88, 493, 738, 146, 1012, 180, 988, 983, 1092, 539, 562, 5, 647, 568, 115, 554, 252, 5, 514, 634, 115, 262, 157, 367, 988, 86, 321, 2, 850, 534, 218, 549, 594, 7, 1, 108, 5, 277, 1094, 533, 2, 850, 711, 644, 5, 1, 876, 851, 5, 1108, 115, 606, 987, 1033, 1016, 544, 539, 705, 9, 157, 53, 453, 539, 218, 988, 660, 33, 722, 988, 520, 5, 157, 1051, 988, 1101, 920, 1108, 982, 715, 780, 153, 449, 180, 371, 600, 7, 55, 988, 339, 290, 722, 539, 225, 5, 152, 115, 799, 722, 323, 157, 352, 719, 5, 561, 729, 722, 988, 427, 740, 246, 5, 568, 988, 571, 482, 473, 7, 93, 668, 739, 722, 988, 473, 242, 1051, 145, 38, 2, 850, 759, 0, 67, 7, 38, 369, 191, 988, 1106, 5, 690, 178, 115, 114, 588, 114, 478, 722, 772, 190, 5, 805, 156, 171, 187, 5, 157, 851, 8, 1, 54, 1126, 922, 540, 1126, 244, 590, 657, 1016, 862, 585, 7, 53, 514, 585, 741, 988, 659, 5, 239, 533, 1121, 2, 970, 620, 585, 925, 7, 1, 112, 6, 53, 292, 590, 657, 1016, 862, 585, 6, 988, 442, 782, 722, 57, 2, 850, 53, 514, 401, 514, 1016, 933, 697, 417, 741, 0, 103, 994, 514, 988, 773, 722, 553, 6, 856, 988, 251, 752, 568, 115, 749, 1124, 734, 114, 846, 32, 114, 365, 5, 734, 115, 682, 374, 774, 908, 987, 585, 1021, 988, 626, 1007, 309, 722, 726, 104, 779, 7, 93, 685, 678, 773, 204, 988, 769, 217, 9, 1125, 5, 177, 697, 417, 511, 126, 1016, 988, 518, 5, 145, 988, 254, 807, 243, 738, 6, 145, 988, 543, 349, 177, 186, 5, 988, 1032, 722, 789, 241, 1093, 5, 1108, 949, 287, 900, 5, 533, 658, 1016, 354, 184, 477, 988, 820, 238, 722, 988, 769, 1016, 910, 790, 582, 722, 333, 7, 67, 7, 38, 5, 788, 115, 706, 960, 1016, 1117, 727, 6, 466, 5, 177, 585, 1088, 5, 908, 572, 988, 192, 722, 539, 742, 769, 6, 514, 619, 542, 568, 156, 1050, 322, 1016, 988, 350, 722, 999, 426, 1068, 7, 93, 769, 1077, 729, 722, 57, 2, 850, 1, 941, 5, 1, 177, 549, 131, 1120, 530, 806, 585, 6, 585, 835, 5, 727, 549, 756, 5, 115, 967, 722, 695, 5, 115, 284, 722, 1063, 5, 115, 194, 5, 931, 157, 934, 5, 987, 833, 729, 722, 988, 267, 2, 850, 580, 380, 1016, 624, 115, 432, 7, 56, 670, 5, 568, 878, 5, 180, 403, 779, 988, 324, 722, 648, 1111, 1016, 198, 746, 1, 942, 1, 205, 876, 1077, 1015, 722, 213, 746, 1, 965, 1, 6, 157, 1125, 711, 1016, 645, 156, 182, 722, 988, 966, 7, 1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7, 1, 93, 602, 239, 729, 5, 1, 876, 291, 542, 6, 1, 239, 988, 735, 356, 2, 970, 294, 5, 205, 533, 330, 585, 7, 1, 1, 29, 585, 10, 1, 53, 1077, 118, 1016, 452, 1051, 999, 273, 1090, 53, 536, 115, 455, 157, 855, 57, 547, 727, 988, 1005, 7, 17, 533, 928, 992, 5, 549, 522, 568, 988, 778, 722, 549, 1064, 274, 5, 988, 995, 235, 1082, 722, 516, 805, 191, 477, 549, 1095, 459, 5, 549, 610, 955, 258, 481, 241, 115, 905, 987, 625, 988, 1014, 722, 115, 865, 689, 5, 53, 436, 1016, 1089, 115, 322, 533, 514, 988, 852, 808, 177, 549, 770, 6, 988, 808, 722, 644, 271, 986, 533, 1077, 7, 51, 1103, 493, 180, 546, 328, 5, 239, 549, 417, 1029, 759, 539, 1016, 988, 782, 7, 1, 66, 7, 83, 1075, 1016, 862, 585, 5, 1, 876, 210, 5, 177, 566, 407, 542, 7, 48, 886, 549, 881, 5, 926, 906, 7, 1, 74, 5, 83, 469, 663, 738, 639, 141, 5, 1, 533, 851, 627, 9, 991, 5, 758, 549, 170, 1007, 674, 8, 1, 27, 157, 862, 988, 840, 722, 988, 559, 7, 1, 48, 883, 585, 1016, 663, 1108, 115, 593, 722, 700, 948, 793, 8, 988, 197, 5, 988, 915, 5, 988, 368, 5, 988, 1034, 6, 145, 988, 282, 893, 722, 988, 673, 2, 850, 358, 377, 7, 14, 1091, 697, 1114, 744, 988, 409, 1030, 533, 851, 5, 1008, 738, 549, 259, 115, 635, 8, 1, 112, 5, 53, 821, 359, 2, 970, 862, 560, 763, 657, 1016, 636, 1109, 987, 7, 1, 108, 6, 585, 1077, 590, 988, 388, 729, 671, 530, 461, 456, 546, 7, 77, 533, 1077, 5, 1007, 585, 145, 157, 568, 917, 722, 585, 145, 6, 177, 533, 514, 208, 1007, 5, 157, 568, 917, 722, 5, 549, 770, 6, 908, 523, 5, 908, 255, 5, 908, 342, 5, 987, 729, 640, 1016, 304, 738, 8, 1, 19, 351, 1108, 1128, 617, 0, 1, 177, 728, 729, 514, 640, 1016, 856, 8, 1, 19, 351, 1108, 1128, 1117, 0, 1, 22, 5, 1108, 988, 304, 727, 697, 633, 5, 697, 334, 951, 156, 1048, 257, 7, 1, 97, 584, 697, 742, 599, 5, 1, 533, 851, 5, 609, 663, 579, 115, 313, 775, 845, 180, 988, 388, 722, 988, 448, 1069, 7, 56, 1077, 919, 157, 235, 157, 613, 8, 709, 1, 379, 1, 9, 709, 230, 5, 710, 722, 988, 143, 722, 783, 456, 836, 568, 115, 769, 1084, 6, 119, 145, 5, 709, 612, 888, 722, 401, 532, 208, 1057, 177, 115, 944, 7, 93, 420, 234, 552, 1016, 663, 988, 121, 439, 722, 57, 2, 850, 227, 1108, 549, 726, 622, 7, 1, 31, 2, 970, 1126, 401, 311, 1108, 745, 161, 685, 10, 1, 53, 179, 5, 926, 644, 118, 456, 115, 1025, 722, 949, 128, 7, 1, 69, 5, 1, 533, 851, 231, 7, 1, 78, 1081, 6, 734, 398, 10, 1, 51, 283, 417, 511, 338, 5, 157, 549, 258, 750, 115, 635, 1044, 989, 523, 954, 7, 1, 69, 998, 722, 585, 5, 697, 319, 435, 6, 161, 685, 986, 566, 53, 2, 310, 707, 1023, 115, 236, 7, 1, 14, 549, 1018, 1017, 663, 568, 115, 446, 987, 533, 707, 1003, 722, 162, 385, 7, 53, 690, 187, 5, 576, 386, 241, 697, 1048, 343, 9, 157, 177, 53, 1037, 5, 697, 415, 434, 727, 115, 904, 769, 119, 988, 659, 6, 988, 731, 718, 228, 988, 775, 716, 753, 722, 988, 845, 7, 1, 74, 5, 241, 58, 0, 1, 53, 851, 7, 56, 1077, 115, 899, 722, 115, 361, 6, 156, 726, 1015, 361, 5, 923, 568, 988, 815, 1044, 115, 1072, 7, 1, 23, 58, 6, 115, 89, 0, 1, 53, 300, 7, 48, 1077, 889, 9, 239, 53, 436, 546, 272, 212, 663, 5, 229, 115, 635, 811, 7, 1, 109, 115, 1114, 0, 61, 1108, 115, 363, 631, 6, 239, 727, 402, 470, 7, 113, 649, 253, 5, 1092, 335, 1126, 488, 585, 10, 1, 48, 160, 903, 8, 1, 67, 7, 89, 484, 585, 1016, 663, 7, 1, 1, 12, 6, 53, 336, 2, 970, 596, 1126, 399, 595, 988, 90, 7, 48, 1077, 949, 156, 573, 541, 7, 1, 1, 53, 336, 2, 970, 6, 1010, 138, 7, 7, 7, 7, 88, 869, 456, 663, 1016, 745, 546, 1090, 533, 1077, 317, 7, 1, 1, 110, 533, 1077, 317, 10, 113, 10, 1, 53, 696, 530, 620, 115, 635, 1020, 691, 151, 396, 1007, 697, 961, 5, 456, 533, 160, 1108, 115, 327, 606, 8, 1, 112, 6, 876, 2, 850, 156, 188, 892, 5, 1126, 596, 5, 67, 7, 89, 7, 49, 731, 563, 1077, 1016, 530, 546, 360, 241, 115, 430, 747, 6, 142, 5, 781, 89, 0, 88, 1003, 585, 988, 959, 1083, 722, 797, 549, 510, 6, 722, 458, 585, 727, 115, 803, 802, 7, 14, 180, 988, 680, 53, 1077, 114, 988, 114, 430, 747, 7, 1, 1, 12, 5, 781, 89, 6, 177, 1126, 856, 7, 106, 114, 987, 114, 549, 550, 10, 1, 1, 92, 1077, 549, 550, 7, 88, 214, 568, 546, 5, 495, 568, 546, 6, 734, 1003, 876, 335, 7, 22, 876, 293, 2, 970, 201, 711, 1016, 530, 145, 988, 366, 1108, 539, 7, 88, 293, 2, 970, 201, 988, 420, 987, 5, 727, 1060, 316, 5, 729, 292, 149, 488, 701, 392, 1016, 862, 549, 770, 7, 80, 1111, 0, 88, 2, 850, 590, 115, 471, 512, 456, 735, 472, 7, 89, 584, 988, 731, 1098, 53, 401, 595, 7, 1, 1, 113, 401, 595, 10, 22, 1126, 590, 851, 6, 1, 38, 514, 115, 307, 905, 568, 549, 417, 7, 1, 74, 5, 53, 595, 546, 5, 157, 533, 595, 663, 6, 731, 585, 526, 138, 533, 1077, 317, 7, 1, 53, 370, 697, 1071, 576, 7, 1, 110, 876, 869, 456, 1126, 10, 1, 1, 112, 6, 813, 574, 1016, 988, 581, 7, 88, 1075, 546, 1067, 6, 157, 241, 663, 0, 1, 48, 607, 140, 5, 157, 1006, 191, 549, 534, 1016, 642, 1051, 180, 988, 899, 722, 988, 361, 7, 1, 95, 1088, 316, 1090, 53, 293, 2, 970, 642, 180, 987, 996, 6, 293, 2, 970, 418, 585, 7, 22, 53, 457, 698, 1016, 806, 585, 540, 9, 157, 714, 585, 2, 850, 305, 663, 6, 305, 663, 7, 92, 2, 850, 988, 823, 1100, 53, 359, 2, 970, 311, 161, 685, 5, 697, 319, 83, 9, 734, 818, 89, 547, 584, 988, 823, 7, 1, 35, 988, 442, 1011, 697, 564, 306, 118, 697, 280, 1037, 579, 115, 870, 329, 1016, 1047, 546, 217, 7, 1, 53, 1107, 1126, 2, 310, 979, 663, 560, 585, 526, 5, 1, 53, 851, 7, 48, 928, 644, 1051, 180, 988, 899, 5, 157, 1041, 218, 549, 441, 115, 263, 533, 514, 463, 1016, 626, 7, 91, 533, 1037, 1024, 663, 7, 1, 53, 2, 310, 818, 628, 1016, 979, 1126, 6, 205, 53, 2, 1062, 149, 964, 1126, 722, 638, 697, 1117, 7, 1, 53, 654, 115, 327, 487, 5, 1093, 533, 703, 1108, 115, 502, 885, 7, 1, 74, 5, 53, 336, 2, 970, 248, 115, 936, 1090, 53, 214, 568, 698, 6, 157, 714, 585, 2, 850, 156, 130, 1009, 218, 1056, 0, 1, 48, 607, 902, 5, 1109, 221, 5, 157, 805, 729, 722, 988, 320, 172, 467, 7, 1, 95, 8, 655, 1129, 278, 6, 157, 540, 169, 988, 264, 1126, 628, 7, 1, 48, 774, 990, 180, 697, 383, 157, 289, 1016, 1073, 1051, 157, 362, 988, 845, 5, 930, 714, 157, 991, 215, 988, 769, 7, 1, 52, 585, 526, 10, 53, 244, 979, 1126, 568, 445, 676, 6, 157, 585, 336, 2, 970, 972, 691, 641, 1016, 525, 7, 7, 7, 7, 53, 244, 832, 714, 560, 962, 157, 777, 53, 1077, 1090, 53, 503, 67, 7, 89, 2, 850, 712, 7, 73, 297, 5, 320, 362, 5, 53, 514, 149, 114, 436, 114, 992, 1077, 709, 729, 628, 546, 6, 731, 53, 514, 499, 1108, 988, 937, 5, 376, 988, 1058, 776, 118, 546, 5, 1010, 53, 517, 503, 1016, 998, 533, 1077, 115, 423, 5, 729, 722, 988, 593, 987, 169, 616, 212, 7, 23, 58, 5, 157, 533, 114, 1077, 114, 616, 212, 6, 205, 533, 514, 277, 1016, 925, 0, 93, 840, 722, 1056, 514, 1016, 620, 737, 198, 968, 148, 734, 497, 1044, 5, 239, 533, 1077, 545, 119, 988, 308, 6, 727, 402, 470, 5, 177, 1126, 856, 7, 1, 108, 5, 53, 1087, 723, 1016, 988, 559, 568, 697, 686, 381, 683, 6, 818, 690, 5, 60, 462, 663, 5, 180, 988, 761, 722, 781, 89, 2, 850, 249, 722, 423, 213, 302, 241, 988, 496, 722, 697, 748, 546, 0, 73, 297, 53, 664, 1016, 355, 988, 769, 456, 713, 6, 53, 1017, 67, 7, 89, 908, 1090, 876, 210, 1016, 921, 912, 118, 539, 785, 7, 53, 832, 489, 723, 115, 798, 768, 118, 988, 553, 213, 114, 674, 114, 6, 725, 5, 53, 1077, 794, 5, 697, 319, 83, 0, 53, 1077, 783, 1016, 698, 628, 729, 722, 697, 742, 898, 7, 1, 94, 53, 1077, 973, 1051, 157, 616, 147, 1108, 546, 7, 53, 514, 869, 145, 697, 1028, 568, 134, 5, 157, 53, 514, 731, 1016, 871, 1051, 988, 374, 157, 488, 1016, 1117, 7, 48, 514, 208, 317, 731, 1039, 558, 5, 157, 533, 337, 950, 5, 722, 537, 348, 5, 908, 987, 992, 514, 208, 709, 787, 1117, 722, 331, 6, 549, 418, 1077, 270, 157, 1049, 7, 53, 514, 670, 546, 728, 734, 1040, 5, 1123, 209, 5, 157, 1003, 546, 575, 157, 340, 7, 71, 53, 855, 987, 533, 1077, 957, 7, 1, 53, 1077, 492, 180, 442, 5, 1108, 115, 669, 135, 853, 8, 492, 1016, 530, 697, 521, 727, 949, 115, 2, 946, 7, 2, 94, 549, 935, 623, 210, 1016, 136, 663, 809, 6, 177, 53, 222, 988, 534, 568, 53, 436, 177, 566, 533, 1088, 1080, 663, 355, 585, 7, 93, 867, 1077, 453, 241, 988, 1003, 8, 566, 533, 114, 1088, 114, 1080, 663, 5, 1089, 1120, 533, 856, 1016, 697, 1083, 722, 1118, 10, 68, 939, 210, 1016, 497, 115, 635, 1104, 6, 53, 436, 704, 157, 1043, 7, 1, 76, 5, 1090, 53, 643, 1051, 5, 53, 863, 1016, 862, 115, 905, 212, 549, 272, 507, 202, 6, 177, 566, 533, 514, 988, 861, 5, 157, 1088, 155, 547, 241, 551, 585, 191, 477, 663, 7, 92, 404, 663, 926, 685, 7, 93, 861, 10, 111, 5, 53, 514, 115, 861, 1119, 1038, 722, 549, 0, 53, 314, 180, 988, 245, 480, 5, 157, 1033, 910, 722, 697, 226, 1032, 7, 22, 994, 422, 663, 5, 994, 303, 7, 53, 855, 987, 533, 1078, 2, 970, 1080, 988, 884, 220, 6, 53, 293, 2, 970, 353, 549, 184, 9, 533, 590, 592, 549, 417, 727, 988, 527, 757, 218, 7, 98, 1088, 988, 730, 53, 514, 149, 877, 5, 734, 298, 1051, 1108, 910, 653, 745, 7, 14, 560, 533, 855, 1007, 697, 621, 0, 1, 53, 643, 1051, 140, 5, 157, 250, 887, 722, 987, 899, 722, 988, 361, 524, 727, 988, 1072, 701, 549, 207, 7, 51, 1103, 1017, 663, 139, 585, 1077, 988, 602, 996, 533, 514, 360, 6, 590, 115, 712, 973, 1108, 115, 874, 521, 5, 1090, 533, 1077, 362, 568, 30, 825, 477, 115, 791, 537, 183, 7, 59, 115, 712, 0, 22, 585, 980, 549, 1098, 550, 7, 95, 169, 1123, 722, 762, 860, 766, 568, 403, 630, 7, 11, 656, 1097, 514, 969, 1108, 988, 308, 292, 707, 530, 611, 987, 672, 1052, 938, 7, 7, 7, 7, 1, 53, 1037, 191, 1016, 697, 1117, 5, 157, 1087, 727, 512, 157, 692, 9, 991, 53, 643, 180, 988, 361, 140, 7, 53, 855, 987, 5, 1090, 89, 598, 568, 988, 442, 938, 5, 533, 595, 590, 1089, 988, 388, 1120, 198, 7, 48, 514, 784, 549, 946, 5, 122, 585, 5, 826, 585, 7, 110, 514, 53, 360, 987, 1108, 161, 722, 697, 997, 10, 96, 515, 2, 970, 208, 223, 722, 663, 6, 53, 514, 590, 132, 990, 7, 7, 7, 7, 1, 46, 585, 5, 83, 5, 1108, 987, 418, 1080, 663, 53, 293, 2, 970, 355, 158, 938, 7, 93, 775, 1036, 1077, 5, 53, 336, 2, 970, 596, 1092, 1016, 806, 585, 6, 114, 53, 514, 707, 597, 114, 7, 77, 5, 1108, 697, 898, 157, 697, 802, 5, 115, 884, 918, 722, 276, 298, 1051, 988, 420, 6, 53, 590, 1006, 745, 579, 989, 419, 7, 7, 7, 7, 108, 5, 745, 1077, 988, 729, 666, 1001, 317, 417, 292, 862, 1007, 6, 862, 932, 1016, 988, 1022, 470, 1046, 7, 31, 2, 970, 1126, 596, 560, 5, 568, 974, 115, 460, 601, 5, 399, 450, 5, 729, 858, 517, 988, 1011, 711, 1089, 729, 1076, 1016, 239, 1089, 729, 244, 10, 108, 6, 987, 1077, 988, 1083, 53, 746, 9, 157, 177, 533, 608, 992, 157, 1079, 663, 5, 988, 996, 994, 242, 697, 2, 978, 2, 275, 628, 115, 559, 722, 247, 7, 48, 336, 2, 970, 907, 5, 1126, 1047, 5, 781, 89, 6, 533, 590, 608, 992, 812, 1080, 5, 157, 727, 549, 633, 5, 1007, 988, 506, 202, 5, 53, 863, 1016, 535, 988, 810, 8, 2, 15, 1126, 958, 1126, 596, 1092, 1126, 2, 819, 279, 738, 10, 2, 1, 54, 53, 292, 530, 746, 987, 418, 5, 1108, 987, 810, 727, 585, 5, 53, 879, 530, 360, 115, 508, 996, 7, 93, 708, 509, 996, 1077, 1016, 862, 987, 53, 293, 2, 970, 6, 157, 987, 504, 1077, 491, 663, 7, 22, 5, 725, 5, 180, 987, 675, 5, 83, 5, 1077, 992, 162, 727, 372, 53, 1121, 2, 970, 530, 491, 1016, 530, 89, 144, 209, 663, 5, 157, 1016, 535, 546, 856, 8, 2, 56, 2, 850, 711, 1020, 603, 6, 53, 2, 637, 882, 1126, 560, 2, 10, 1, 56, 114, 1077, 114, 1020, 603, 6, 585, 1120, 530, 208, 5, 399, 566, 533, 2, 310, 208, 144, 7, 53, 743, 1051, 697, 1028, 5, 157, 1087, 362, 157, 1017, 67, 7, 89, 7, 73, 297, 53, 336, 2, 970, 979, 539, 114, 987, 114, 6, 585, 1120, 530, 208, 41, 1016, 539, 7, 53, 894, 851, 53, 293, 2, 970, 745, 546, 5, 987, 53, 1077, 1020, 690, 7, 88, 818, 629, 988, 563, 6, 876, 2, 850, 908, 844, 0, 56, 1077, 987, 987, 654, 539, 490, 663, 988, 361, 7, 22, 876, 1077, 985, 1054, 180, 711, 489, 988, 782, 6, 876, 335, 908, 1074, 546, 2, 360, 2, 241, 910, 729, 884, 0, 18, 442, 53, 1077, 137, 876, 1121, 2, 970, 620, 663, 723, 6, 157, 180, 697, 1110, 2, 388, 53, 953, 42, 7, 112, 5, 585, 1077, 53, 1097, 924, 42, 8, 53, 1017, 67, 7, 89, 533, 1077, 988, 2, 279, 2, 656, 5, 157, 876, 1017, 911, 385, 5, 157, 908, 585, 503, 1016, 198, 1035, 7, 7, 7, 7, 14, 533, 746, 89, 1109, 1105, 9, 157, 876, 561, 988, 769, 153, 539, 562, 2, 850, 997, 7, 7, 7, 7, 1, 48, 451, 547, 362, 568, 988, 171, 701, 674, 5, 598, 191, 549, 534, 5, 157, 269, 549, 173, 215, 585, 5, 643, 1051, 180, 988, 769, 119, 988, 260, 7, 1, 53, 628, 1016, 429, 987, 89, 547, 1120, 530, 491, 585, 1016, 663, 5, 566, 533, 2, 310, 208, 117, 1016, 856, 1089, 533, 1003, 987, 315, 7, 1, 14, 5, 568, 159, 1016, 115, 810, 53, 806, 519, 6, 1, 20, 140, 10, 1, 533, 447, 738, 7, 1, 110, 988, 729, 996, 987, 232, 663, 163, 701, 546, 584, 987, 53, 595, 392, 1016, 614, 723, 10, 1, 48, 928, 1051, 157, 598, 549, 521, 727, 697, 880, 1108, 115, 606, 7, 1, 77, 988, 581, 722, 585, 584, 987, 53, 114, 150, 114, 926, 748, 6, 895, 42, 2, 850, 357, 585, 456, 663, 0, 93, 90, 922, 147, 5, 157, 525, 728, 6, 239, 992, 2, 850, 709, 413, 736, 593, 722, 174, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2cb8ff09-d42e-403f-a1cf-9c0ddaee59d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [53, 44, 149, 1003]\n",
      "y:      [44, 149, 1003, 57]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "918f6f99-bb89-44c0-9ee8-e1d0ef030ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53] ----> 44\n",
      "[53, 44] ----> 149\n",
      "[53, 44, 149] ----> 1003\n",
      "[53, 44, 149, 1003] ----> 57\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a34ab3c0-2fb2-4037-894b-601f57a95e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ----> HAD\n",
      "I HAD ----> always\n",
      "I HAD always ----> thought\n",
      "I HAD always thought ----> Jack\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28291265-0320-4ff2-84b7-5c9b7ab894ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1569f3ce-2734-4c62-a9a3-34732440f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "tokenizer  = tiktoken.get_encoding(\"o200k_harmony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eae7b40a-380f-4d96-b3dc-345f13a822f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12194, 357, 939, 23967, 368]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode((\"Hi I am Vinod\"), allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35030f22-98c5-45e0-8fad-9d0ecf7cebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'r50k_base',\n",
       " 'p50k_base',\n",
       " 'p50k_edit',\n",
       " 'cl100k_base',\n",
       " 'o200k_base',\n",
       " 'o200k_harmony']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1d25f23-2bbd-4870-b105-0b90b4791b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tourch (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tourch\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tourch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d12e1711-b9cd-4437-bf98-82cb3f9a7081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1aeea965-ddf5-4f65-a2d9-8b64fc9cbbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset): \n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #Using a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8eb3f96-7c30-422f-bec2-f5849e961ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a764127-7b4a-4181-816b-e281ddfee463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43d06b56-7dca-4d91-8454-7ce60c797650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f672e29d-53bf-469a-9026-4e84e0444fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a5be7c8e-90e7-4165-84a5-fe06ac90dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9fbf877-1b5e-43ef-9e64-199fb7d66643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ba4d95a-dea8-4a66-8f13-effa31b5572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2025.11.12)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b47a5e4c-bac2-4e30-9dbf-899420855ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "model = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79371639-abda-47ee-8a99-468227ce9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f286c848-090e-4075-9603-7fe6916a9c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
     ]
    }
   ],
   "source": [
    "print(word_vector['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d78c8b83-006b-4b5a-a512-bd29411d2416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(word_vector['cat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9bf88a1a-5f4e-45b6-91f8-4f41139c4412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.4827326238155365), ('queens', 0.466781347990036), ('kumaris', 0.4653734564781189), ('kings', 0.4558638036251068), ('womens', 0.422832190990448), ('princes', 0.4176960587501526), ('Al_Anqari', 0.41725507378578186), ('concubines', 0.4011078476905823), ('monarch', 0.3962482810020447), ('monarchy', 0.39430147409439087)]\n"
     ]
    }
   ],
   "source": [
    "# King + Women - Man = ?\n",
    "print(word_vector.most_similar(positive=['king', 'women'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "72b2fcf2-e4c2-486b-9437-ad6489fb8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n",
      "0.6510956\n",
      "0.76434743\n",
      "0.8543272\n",
      "0.7594367\n",
      "0.11408083\n"
     ]
    }
   ],
   "source": [
    "print(word_vector.similarity('woman', 'man'))\n",
    "print(word_vector.similarity('king', 'queen'))\n",
    "print(word_vector.similarity('uncle', 'aunt'))\n",
    "print(word_vector.similarity('boy', 'girl'))\n",
    "print(word_vector.similarity('nephew', 'niece'))\n",
    "print(word_vector.similarity('paper', 'water'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "424e2a19-3d9d-406e-9434-6ed12db5c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tome', 0.7485831379890442), ('books', 0.7379178404808044), ('memoir', 0.7302927374839783), ('paperback_edition', 0.6868364810943604), ('autobiography', 0.6741527915000916)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vector.most_similar(\"book\", topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf661020-a471-40d8-9dbe-17e09ccbcaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the difference between 'man' and 'woman' is 1.73\n",
      "The magnitude of the difference between 'semiconductor' and 'earthworm' is 5.67\n",
      "The magnitude of the difference between 'nephew' and 'niece' is 1.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Words to compare\n",
    "word1 = 'man'\n",
    "word2 = 'woman'\n",
    "\n",
    "word3 = 'semiconductor'\n",
    "word4 = 'earthworm'\n",
    "\n",
    "word5 = 'nephew'\n",
    "word6 = 'niece'\n",
    "\n",
    "# Calculate the vector difference\n",
    "vector_difference1 = model[word1] - model[word2]\n",
    "vector_difference2 = model[word3] - model[word4]\n",
    "vector_difference3 = model[word5] - model[word6]\n",
    "\n",
    "# Calculate the magnitude of the vector difference\n",
    "magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
    "magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
    "magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
    "\n",
    "\n",
    "# Print the magnitude of the difference\n",
    "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word1, word2, magnitude_of_difference1))\n",
    "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word3, word4, magnitude_of_difference2))\n",
    "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word5, word6, magnitude_of_difference3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "212eb9b0-3c3a-43a4-ae77-cacf3bfadbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dc3bf282-e3bf-44d0-9be1-bd8a47165edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42668230-723b-4064-b68a-2dae41818ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "057c62d6-a107-4a0d-86cf-ca25650ba854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c3723900-1985-48ae-a54e-60e30c21cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f012c37-1e6a-412c-b6f8-be6f6d8851e7",
   "metadata": {},
   "source": [
    " IMPLEMENTING A SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff2b46d7-8acd-4b99-bc06-a1a1ebbadfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd51db3c-b3e1-4341-bfed-81b8374d7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] #A\n",
    "print(x_2)\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "88b72e4f-951c-411b-8055-e60eecc56d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "print(W_query)\n",
    "print(W_key)\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9b0e8b23-2959-4fc6-9eeb-b55d2ae6657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac1ab9fb-1c4e-4750-8f55-7b2ab32e61b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys.shape torch.Size([6, 2])\n",
      "Values.shape torch.Size([6, 2])\n",
      "Queries.shape torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "\n",
    "print(\"Keys.shape\", keys.shape)\n",
    "print(\"Values.shape\", values.shape)\n",
    "print(\"Queries.shape\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4704582-75d6-4522-8793-f9f4f2205bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 5.0473323512129005\n",
      "Variance after scaling (dim=5): 1.00946647024258\n",
      "Variance before scaling (dim=1000): 940.9565764957678\n",
      "Variance after scaling (dim=1000): 0.9409565764957678\n"
     ]
    }
   ],
   "source": [
    "#But why sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000): \n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "    \n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "    \n",
    "        #Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product/np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 1000\n",
    "variance_before_1000, variance_after_1000 = compute_variance(1000)\n",
    "print(f\"Variance before scaling (dim=1000): {variance_before_1000}\")\n",
    "print(f\"Variance after scaling (dim=1000): {variance_after_1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacfb4a1-2def-4345-9bda-60abe7b96264",
   "metadata": {},
   "source": [
    "HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0fd9e6fb-544d-4663-a53c-f832b4c31d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0410495e-f385-4253-b850-4d0786a9cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ac416be1-6d60-42db-9cb8-37338fcc2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6f765943-ef76-4e1e-b1b4-f2977c1b4b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "59cc3580-d62d-4593-9733-3b5a6011bd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ef886cf4-266c-41cd-89af-75a54578e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50610a58-0059-411d-9ae4-4ef3e4fd5786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple/row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67053a5-b511-4fb2-a65e-6f4f347744da",
   "metadata": {},
   "source": [
    "Right way of masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9a78093e-5d78-4839-bb86-5c91eb3197a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "54d4eb2d-1f13-419f-bf8c-b4d257793e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8ed65a01-1489-4aa5-ab87-731e4670f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e420a657-659c-4a81-ba6f-61cdf5640613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c25a7-8481-4f42-8492-5418f6322a59",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9669bb4b-93ac-46f5-96a7-9fbae8597afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "409603c1-95b5-48cd-a67e-1ee6f377d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ba1e208-0377-4e93-bef2-638ed86a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "777362dd-0c02-4579-8f57-e0a1f91a9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "952b6745-e3ed-4217-b467-4864a1168e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "context_length = 6\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "57317de8-8929-492c-be13-5bd1845895ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ff598ca7-5bbe-4b50-b9cb-19fa7758a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0a6e0165-d810-4b14-b81b-684d157d94c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ded1a3a4-2733-4e6f-9f33-81ac5e359bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a acausal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dcfbe1c1-3a2c-43e2-855a-c70b9e04fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "36747133-81e5-40eb-979f-52e27aef953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ce1131f6-781a-40d4-89be-d4e5ec8d9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e5d1be15-53c0-4be2-a2c0-75c870702229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance \\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "53b6adf0-a443-433b-862c-c02c1923392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "811dc46b-fd89-4ed4-81a4-1dc574a07749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "#SAMPLE DATA\n",
    "x = torch.linspace(-3,3,100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f{label} (x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d996d432-52e4-41d0-8500-d187ddece073",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bbdced31-26d7-4d9f-91ae-e7da79aa9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), GELU(), nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ab24b44-941a-4404-91ea-9e66dc1b6efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1d923-e519-455d-94f9-ad5db413b7de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">GPT ARCHITECTURE PART 4: SHORTCUT CONNECTIONS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "28985ac2-cbc7-4f78-8a78-13b59e1b4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of current \n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3b5c6577-b4bc-4780-ba47-b73f7933d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "simple_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "33d04106-ca57-4017-9137-13d9a4893bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840f9ff-13f6-40f8-a3fa-56bb47d825b8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">GPT ARCHITECTURE PART 5: coding attention and linear layers block</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4a20f0a8-636e-4ef9-b31a-beb49b8179c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fe9d7b31-b992-4d8d-bb0f-8463b229443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forwared block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9dfe7a2c-f146-4980-913b-3ee8de11312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "388d2e27-f950-4edb-bcf6-d9619b5cf130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds+pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5fcd1af7-7d3a-4d68-9c9a-c8542c127c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c40e61a5-1e0f-4035-b10b-f2ad8cce1163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6b1ddc11-a5bf-4ae2-a52b-6c5669a26ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0b1bc29a-bfcf-4558-b4d4-9e33105a9b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9757befe-87d0-4e01-a551-5e836afd739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight : 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight : {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9bcd18fb-b520-41be-bdbd-6db65c8b19ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "994d8729-2829-4597-babe-7651692b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:,-1,:]\n",
    "    \n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1) #(batch, vocab_size)\n",
    "    \n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (batch, 1)\n",
    "    \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) #(batch, n_tokens+1)\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3050e473-238f-466e-9791-50961bd0521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded: \", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ae78a2f5-b190-426c-8449-095f9d9c6176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output  tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length:  10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = generate_text_simple(model=model, idx=encoded_tensor, max_new_tokens=6, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output \", out)\n",
    "print(\"Output length: \", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "96b9e4b8-cc81-419a-9fbb-ec8128e0fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808cf53-9eb3-4029-b114-127ccce32485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
